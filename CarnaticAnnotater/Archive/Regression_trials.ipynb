{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import crepe\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import argrelextrema\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import IPython.display as ipd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from scipy.signal import find_peaks as scipy_find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "end_time = 30\n",
    "\n",
    "audio_path = r\"C:\\Users\\nandh\\Downloads\\Varnams\\split sounds\\223582__gopalkoduri__carnatic-varnam-by-vignesh-in-abhogi-raaga\\vocals.wav\"\n",
    "csv_path = \"Evari Bodhana.csv\"\n",
    "y, sr = librosa.load(audio_path, sr=44100, offset=start_time, duration=end_time - start_time,mono=True)\n",
    "\n",
    "\n",
    "# Compute spectrogram\n",
    "D = librosa.stft(y)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.min)\n",
    "\n",
    "# CREPE pitch estimation\n",
    "time, frequency, confidence, activation = crepe.predict(y, sr, viterbi=True, step_size=20, model_capacity=\"tiny\")\n",
    "\n",
    "# # Interpolation\n",
    "spec_time = librosa.times_like(D, sr=sr)\n",
    "interp_freq = interp1d(time, frequency, kind='linear', fill_value='extrapolate')\n",
    "interp_conf = interp1d(time, confidence, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "new_frequency = interp_freq(spec_time)\n",
    "new_confidence = interp_conf(spec_time)\n",
    "\n",
    "df = pd.DataFrame({\"Time\": spec_time, \"Frequency\": new_frequency, \"Confidence\": new_confidence})\n",
    "with open(csv_path, 'w') as f:\n",
    "    f.truncate(0)  # Clear file\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"CSV file cleared and saved at {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confidence = pd.read_csv(csv_path)[\"Confidence\"].values\n",
    "new_frequency = pd.read_csv(csv_path)[\"Frequency\"].values\n",
    "spec_time = pd.read_csv(csv_path)[\"Time\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram_with_crepe(spec_time, conf, S_db, sr):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='linear', cmap='viridis')\n",
    "    plt.plot(spec_time, conf, color='r', linewidth=1.5, label='CREPE Pitch')  # Use spec_time\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.ylim(0, 2000)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def find_tonic(S, sr):\n",
    "    chroma = librosa.feature.chroma_stft(S=np.abs(S), sr=sr)\n",
    "    pitch_class_sums = np.sum(np.abs(chroma), axis=1)\n",
    "    pitch_labels = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    pitch_class_dict = dict(zip(pitch_labels, pitch_class_sums))\n",
    "    return max(pitch_class_dict, key=pitch_class_dict.get)\n",
    "\n",
    "def get_carnatic_frequencies(tonic):\n",
    "    # Intonational ratios for the basic set of Carnatic notes\n",
    "    carnatic_ratios = {\n",
    "        'sa': 0.5*1.0,    # Tonic (Sa)\n",
    "        'ri1': 0.5*16/15, # Ri1\n",
    "        'ri2': 0.5*9/8,  # Ri2\n",
    "        'ga1': 0.5*6/5,  # Ga1\n",
    "        'ga2': 0.5*5/4, # Ga2\n",
    "        'ma1': 0.5*4/3, # Ma1\n",
    "        'ma2': 0.5*45/32,   # Ma2\n",
    "        'pa': 0.5*3/2,    # Pa\n",
    "        'da1': 0.5*8/5, # Dha1\n",
    "        'da2': 0.5*5/3, # Dha2\n",
    "        'ni1': 0.5*16/9, # Ni1\n",
    "        'ni2': 0.5*15/8,   # Ni2\n",
    "\n",
    "        'Sa': 1.0,    # Tonic (Sa)\n",
    "        'Ri1': 16/15, # Ri1\n",
    "        'Ri2': 9/8,  # Ri2\n",
    "        'Ga1': 6/5,  # Ga1\n",
    "        'Ga2': 5/4, # Ga2\n",
    "        'Ma1': 4/3, # Ma1\n",
    "        'Ma2': 45/32,   # Ma2\n",
    "        'Pa': 3/2,    # Pa\n",
    "        'Da1': 8/5, # Dha1\n",
    "        'Da2': 5/3, # Dha2\n",
    "        'Ni1': 16/9, # Ni1\n",
    "        'Ni2': 15/8,   # Ni2\n",
    "\n",
    "        'SA': 2.0,   # Octave higher (Sa)\n",
    "        'RI1': 2*16/15, # Ri1\n",
    "        'RI2': 2*9/8,  # Ri2\n",
    "        'GA1': 2*6/5,  # Ga1\n",
    "        'GA2': 2*5/4, # Ga2\n",
    "        'MA1': 2*4/3, # Ma1\n",
    "        'MA2': 2*45/32,   # Ma2\n",
    "        'PA': 2*3/2,    # Pa\n",
    "        'DA1': 2*8/5, # Dha1\n",
    "        'DA2': 2*5/3, # Dha2\n",
    "        'NI1': 2*16/9, # Ni1\n",
    "        'NI2': 2*15/8,   \n",
    "    }\n",
    "\n",
    "    tonic_freq = librosa.note_to_hz(tonic)  # Get the frequency of the tonic\n",
    "\n",
    "    # Calculate the frequencies for each Carnatic note relative to the tonic\n",
    "    carnatic_frequencies = {note: tonic_freq * ratio for note, ratio in carnatic_ratios.items()}\n",
    "    return carnatic_frequencies\n",
    "\n",
    "def get_closest_note(freq, carnatic_frequencies):\n",
    "    \"\"\"Find the closest Carnatic note for a given frequency.\"\"\"\n",
    "    return min(carnatic_frequencies, key=lambda note: abs(carnatic_frequencies[note] - freq))\n",
    "\n",
    "def get_closest_frequency(freq, carnatic_frequencies):\n",
    "    \"\"\"Find the closest Carnatic note frequency for a given frequency.\"\"\"\n",
    "    return min(carnatic_frequencies.values(), key=lambda f: abs(f - freq))\n",
    "\n",
    "def get_index_from_time(time_input,conf):\n",
    "    # Define the start and end times\n",
    "    total_duration = end_time - start_time\n",
    "    num_pieces = len(conf)\n",
    "    \n",
    "    # Calculate the duration of each piece\n",
    "    duration_per_piece = total_duration / num_pieces\n",
    "    \n",
    "    # Check if the input time is within the valid range\n",
    "    if time_input < start_time or time_input > end_time:\n",
    "        raise ValueError(f\"Input time must be between {start_time} and {end_time} seconds.\")\n",
    "    \n",
    "    # Calculate the index\n",
    "    index = int((time_input - start_time) / duration_per_piece)\n",
    "    \n",
    "    return index\n",
    "\n",
    "def plot_frequency_with_carnatic_notes(frequency_list, beat_frames, tonic,beat_sr):\n",
    "    beat_frames= librosa.frames_to_time(beat_frames, sr=beat_sr)\n",
    "    loc_extremes = np.where(np.diff(np.sign(np.diff(frequency_list, prepend=np.nan, append=np.nan))) != 0)[0]\n",
    "    extremes = frequency_list[loc_extremes].tolist()\n",
    "    angles = np.degrees(np.arctan(np.diff(frequency_list, prepend=np.nan, append=np.nan) / 2))\n",
    "    # notelist = [(conf[i], i, angles[i], angles[i + 1], i in loc_extremes) for i in range(len(conf) - 1)]\n",
    "    carnatic_frequencies = get_carnatic_frequencies(tonic)\n",
    "    frequency_array = np.array(frequency_list)\n",
    "    \n",
    "    beat_points=[]\n",
    "    for i in beat_frames:\n",
    "        if i < start_time or i > end_time:\n",
    "            continue\n",
    "        beat_points.append(get_index_from_time(i,frequency_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Identify valid (non-NaN) frames\n",
    "    valid_indices = ~np.isnan(frequency_array)  \n",
    "    valid_frequencies = frequency_array[valid_indices]\n",
    "    if len(valid_frequencies) == 0:\n",
    "        raise ValueError(\"No valid frequencies to process.\")\n",
    "\n",
    "    carnatic_frequencies = get_carnatic_frequencies(tonic)\n",
    "\n",
    "    # Plot the graph\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot the frequency graph with gaps for NaNs\n",
    "    for start, end in zip(\n",
    "        np.where(np.diff(np.concatenate(([0], valid_indices, [0]))) == 1)[0],\n",
    "        np.where(np.diff(np.concatenate(([0], valid_indices, [0]))) == -1)[0]\n",
    "    ):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.arange(start, end),\n",
    "            y=frequency_array[start:end],\n",
    "            mode='lines',\n",
    "            name='Frequency (Hz)',\n",
    "            line=dict(color='blue')\n",
    "        ))\n",
    "\n",
    "    # Plot horizontal lines for Carnatic notes\n",
    "    for note, freq in carnatic_frequencies.items():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[0, len(frequency_list) - 1],\n",
    "            y=[freq, freq],\n",
    "            mode='lines',\n",
    "            line=dict(dash='dash', color='gray', width=2),\n",
    "            name=note,\n",
    "            hovertemplate=f\"{note} ({freq:.2f} Hz)\"\n",
    "        ))\n",
    "\n",
    "    # Plot the extremes as red dots\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=loc_extremes,\n",
    "        y=extremes,\n",
    "        mode='markers',\n",
    "        marker=dict(color='red', size=2, symbol='circle'),\n",
    "        name='Extremes'\n",
    "    ))\n",
    "\n",
    "    # Plot vertical lines for beat points\n",
    "    for beat in beat_points:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[beat, beat],  # Vertical line at 'beat'\n",
    "            y=[np.nanmin(frequency_array), np.nanmax(frequency_array)],  # Full y-range\n",
    "            mode='lines',\n",
    "            line=dict(color='orange', width=2),\n",
    "            name=f'Beat @ {beat}'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Frequency with Carnatic Notes (Tonic: {tonic})',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Frequency (Hz)',\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def breaklist(elements, indexes):\n",
    "    segmented_lists = []\n",
    "    start_index = 0  \n",
    "\n",
    "    for idx in indexes:\n",
    "        segment = elements[start_index:idx]\n",
    "        segmented_lists.append(segment)\n",
    "        start_index = idx  \n",
    "    if start_index < len(elements):\n",
    "        segmented_lists.append(elements[start_index:])\n",
    "\n",
    "    return segmented_lists\n",
    "\n",
    "def plot_with_carnatic_bars(note_num, noteslist, carnatic_frequencies):\n",
    "    bars = list(carnatic_frequencies.values())\n",
    "    \n",
    "    # Find relevant frequency range\n",
    "    min_freq = get_closest_frequency(np.nanmin(noteslist[note_num]), carnatic_frequencies)\n",
    "    max_freq = get_closest_frequency(np.nanmax(noteslist[note_num]), carnatic_frequencies)\n",
    "    \n",
    "    # Filter bars within the frequency range\n",
    "    newbars = [i for i in bars if min_freq <= i <= max_freq]\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(noteslist[note_num])\n",
    "    for i in newbars:\n",
    "        plt.axhline(y=i, color='r', linestyle='--')\n",
    "    plt.show()\n",
    "    for i in newbars:\n",
    "        print(get_closest_note(i, carnatic_frequencies))\n",
    "\n",
    "def spectral_decomp(note, n_clusters, plot=True):\n",
    "    note = np.array(note)\n",
    "    X = np.column_stack((np.arange(len(note)), note))\n",
    "    embedding = SpectralEmbedding(n_components=2, affinity='nearest_neighbors')\n",
    "    X_transformed = embedding.fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_transformed)\n",
    "\n",
    "    # Sort clusters based on first occurrence\n",
    "    unique_clusters = np.unique(labels, return_index=True)\n",
    "    sorted_clusters = [cluster for _, cluster in sorted(zip(unique_clusters[1], unique_clusters[0]))]\n",
    "    label_mapping = {old: new for new, old in enumerate(sorted_clusters)}\n",
    "    sorted_labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "    # Assign frequencies to clusters\n",
    "    segments = [[] for _ in range(n_clusters)]\n",
    "    for idx, freq in enumerate(note):\n",
    "        segments[sorted_labels[idx]].append((idx, freq))\n",
    "\n",
    "    if plot:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan', 'magenta']\n",
    "        for i in range(n_clusters):\n",
    "            indices, freqs = zip(*segments[i]) if segments[i] else ([], [])\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=indices,\n",
    "                y=freqs,\n",
    "                mode='markers+lines',\n",
    "                marker=dict(size=6, color=colors[i % len(colors)]),\n",
    "            ))\n",
    "\n",
    "        # Plot horizontal lines at each unique frequency\n",
    "\n",
    "        unique_freqs = [i for i in get_carnatic_frequencies(\"C#3\").values() if min(note) <= i <= max(note)]\n",
    "        unique_notes= [i for i in get_carnatic_frequencies(\"C#3\").keys() if min(note) <= get_carnatic_frequencies(\"C#3\")[i] <= max(note)]\n",
    "        x_values = np.linspace(min(X[:, 0]), max(X[:, 0]), num=100)  # Densely spaced x values\n",
    "\n",
    "        for i in range (len( unique_freqs)):\n",
    "            y_values = np.full_like(x_values,unique_freqs[i])\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_values,\n",
    "                y=y_values,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"gray\", dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "                hovertemplate=f\"{unique_notes[i]}({unique_freqs[i]:.2f} Hz)\"\n",
    "            ))\n",
    "\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "    return [list(zip(*seg))[1] if seg else [] for seg in segments] \n",
    "\n",
    "def playnote(n, beat_audio, beat_sr, beat_times, start_time):\n",
    "    adjusted_beat_times = beat_times - start_time\n",
    "    adjusted_beat_times = adjusted_beat_times[adjusted_beat_times >= 0]  # Remove negative times\n",
    "    if n < 0 or n >= len(adjusted_beat_times) - 1:\n",
    "        print(\"Invalid note index\")\n",
    "        return\n",
    "    note_start_time = adjusted_beat_times[n]\n",
    "    note_end_time = adjusted_beat_times[n+1]\n",
    "    start_sample = int(note_start_time * beat_sr)\n",
    "    end_sample = int(note_end_time * beat_sr)\n",
    "\n",
    "    note_audio = beat_audio[start_sample:end_sample]\n",
    "    ipd.display(ipd.Audio(note_audio, rate=beat_sr))\n",
    "\n",
    "def find_peaks_and_valleys(conf):\n",
    "    peaks = []\n",
    "    valleys = []\n",
    "    \n",
    "    for i in range(1, len(conf) - 1):\n",
    "        if not np.isnan(conf[i-1]) and not np.isnan(conf[i]) and not np.isnan(conf[i+1]):\n",
    "            if conf[i] > conf[i-1] and conf[i] > conf[i+1]:\n",
    "                peaks.append(i)\n",
    "            elif conf[i] < conf[i-1] and conf[i] < conf[i+1]:\n",
    "                valleys.append(i)\n",
    "    \n",
    "    return peaks, valleys\n",
    "\n",
    "def play_segment_between_beats(beat_audio, beat_sr, beat_frames, beat_index,offset=0):\n",
    "    # Ensure the beat_index is valid\n",
    "    if beat_index < 0 or beat_index >= len(beat_frames) - 1:\n",
    "        print(\"Invalid beat index. Please provide a valid index.\")\n",
    "        return\n",
    "\n",
    "    # Get the start and end frames for the segment\n",
    "    start_frame = beat_frames[beat_index-offset]\n",
    "    end_frame = beat_frames[beat_index + 1+offset]\n",
    "\n",
    "    # Convert frames to time\n",
    "    start_time = librosa.frames_to_time(start_frame, sr=beat_sr)\n",
    "    end_time = librosa.frames_to_time(end_frame, sr=beat_sr)\n",
    "\n",
    "    # Convert time to sample indices\n",
    "    start_sample = int(start_time * beat_sr)\n",
    "    end_sample = int(end_time * beat_sr)\n",
    "\n",
    "    # Slice the audio segment\n",
    "    audio_segment = beat_audio[start_sample:end_sample]\n",
    "\n",
    "    # Play the audio segment\n",
    "    ipd.display(ipd.Audio(audio_segment, rate=beat_sr))\n",
    "\n",
    "def trim(data):\n",
    "    data = np.array(data)  \n",
    "    valid_indices = np.where(~np.isnan(data))[0]\n",
    "    valid_data = data[valid_indices]\n",
    "    peaks = argrelextrema(valid_data, np.greater, order=2)[0]\n",
    "\n",
    "    troughs = argrelextrema(valid_data, np.less, order=2)[0]\n",
    "\n",
    "    # Combine peaks & troughs and sort them\n",
    "    extrema = np.sort(np.concatenate((peaks, troughs)))\n",
    "\n",
    "    if len(extrema) < 2:\n",
    "        return data  # Not enough peaks/troughs to trim\n",
    "\n",
    "    # Find start and end positions in original indices\n",
    "    start, end = valid_indices[extrema[0]], valid_indices[extrema[-1]]\n",
    "\n",
    "    return data[start:end+1]\n",
    "\n",
    "def shift_beats_to_peaks_or_valleys(beat_frames, conf):\n",
    "    \"\"\"\n",
    "    Shift the beat frames to align with the nearest peak or valley in the confidence array.\n",
    "    \n",
    "    Parameters:\n",
    "    - beat_frames: The original beat frames.\n",
    "    - conf: The confidence array.\n",
    "    \n",
    "    Returns:\n",
    "    - shifted_beat_frames: The updated beat frames.\n",
    "    \"\"\"\n",
    "    peaks, valleys = find_peaks_and_valleys(conf)\n",
    "    shifted_beat_frames = []\n",
    "\n",
    "    for beat in beat_frames:\n",
    "        # Find the nearest peak or valley\n",
    "        nearest_index = None\n",
    "        min_distance = float('inf')\n",
    "\n",
    "        for index in peaks + valleys:\n",
    "            distance = abs(index - beat)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_index = index\n",
    "\n",
    "        shifted_beat_frames.append(nearest_index)\n",
    "\n",
    "    return np.array(shifted_beat_frames)\n",
    "\n",
    "def extend_sublists(main_list, num=4):\n",
    "    extended_list = []\n",
    "    for i in range(len(main_list)):\n",
    "        current_sublist = main_list[i]\n",
    "        if i == 0 or i == len(main_list) - 1:\n",
    "            extended_list.append(current_sublist)\n",
    "        else:\n",
    "            new_sublist = []\n",
    "            new_sublist.extend(main_list[i - 1][-num:])\n",
    "            new_sublist.extend(current_sublist)\n",
    "            new_sublist.extend(main_list[i + 1][:num])\n",
    "            extended_list.append(new_sublist)\n",
    "    return extended_list\n",
    "\n",
    "def plot_with_carnatic_bars_with_peaks(note_num, noteslist, carnatic_frequencies):\n",
    "    bars = list(carnatic_frequencies.values())\n",
    "\n",
    "    # Find relevant frequency range\n",
    "    min_freq = get_closest_frequency(np.nanmin(noteslist[note_num]), carnatic_frequencies)\n",
    "    max_freq = get_closest_frequency(np.nanmax(noteslist[note_num]), carnatic_frequencies)\n",
    "    newbars = [i for i in bars if min_freq <= i <= max_freq]\n",
    "\n",
    "    data = noteslist[note_num]\n",
    "    plt.scatter(np.arange(len(data)), data,s=1, color='green')\n",
    "\n",
    "    peaks, _ = scipy_find_peaks(data)\n",
    "    valleys, _ = scipy_find_peaks(-np.array(data))  # Negate to find valleys\n",
    "\n",
    "    plt.plot(peaks, data[peaks], \"bo\", markersize=4)\n",
    "    plt.plot(valleys, data[valleys], \"bo\", markersize=4)\n",
    "\n",
    "    for i in newbars:\n",
    "        plt.axhline(y=i, color='r', linestyle='--')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    for i in newbars:\n",
    "        print(get_closest_note(i, carnatic_frequencies))\n",
    "\n",
    "def interpolate_with_nans(data, target_length=128):\n",
    "\n",
    "    data = np.array(data, dtype=np.float64)\n",
    "    original_length = len(data)\n",
    "    x_original = np.linspace(0, 1, original_length)\n",
    "    x_target = np.linspace(0, 1, target_length)\n",
    "    valid = ~np.isnan(data)\n",
    "    if np.count_nonzero(valid) < 2:\n",
    "        return np.full(target_length, np.nan)\n",
    "    interpolator = interp1d(x_original[valid], data[valid], kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "    interpolated = interpolator(x_target)\n",
    "    nan_mask_original = np.isnan(data)\n",
    "    nan_mask_interpolated = np.interp(x_target, x_original, nan_mask_original.astype(float)) > 0.5\n",
    "    interpolated[nan_mask_interpolated] = np.nan\n",
    "    return interpolated\n",
    "\n",
    "def play_segment(beat_audio, beat_sr, start_frame,end_frame):\n",
    "    # Ensure the beat_index is valid\n",
    "\n",
    "\n",
    "    # Convert frames to time\n",
    "    start_time = librosa.frames_to_time(start_frame, sr=beat_sr)\n",
    "    end_time = librosa.frames_to_time(end_frame, sr=beat_sr)\n",
    "\n",
    "    # Convert time to sample indices\n",
    "    start_sample = int(start_time * beat_sr)\n",
    "    end_sample = int(end_time * beat_sr)\n",
    "\n",
    "    # Slice the audio segment\n",
    "    audio_segment = beat_audio[start_sample:end_sample]\n",
    "\n",
    "    # Play the audio segment\n",
    "    ipd.display(ipd.Audio(audio_segment, rate=beat_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonic =  find_tonic(D, sr)\n",
    "print(tonic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]\n",
    "conf_thresh = 0  # Confidence threshold\n",
    "rmse_thresh = np.percentile(rmse, 7)  # Reject bottom 20% of RMSE energy\n",
    "\n",
    "\n",
    "conf = np.where(new_confidence > conf_thresh, new_frequency, np.nan)\n",
    "conf[rmse < rmse_thresh] = np.nan  # Reject frames with low energy\n",
    "\n",
    "\n",
    "print(len(conf))\n",
    "plot_spectrogram_with_crepe(spec_time, conf, S_db, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beat_audio_path = r\"C:\\Users\\nandh\\Downloads\\carnatic_varnam_1.1\\carnatic_varnam_1.1\\Audio\\223582__gopalkoduri__carnatic-varnam-by-vignesh-in-abhogi-raaga.mp3\"\n",
    "beat_audio, beat_sr = librosa.load(beat_audio_path, sr=None, mono=True)\n",
    "\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=beat_audio, sr=beat_sr, tightness=400)\n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=beat_sr)\n",
    "audio_beat_clicks = librosa.clicks(times=beat_times, sr=beat_sr, click_freq=1500, length=len(beat_audio))\n",
    "\n",
    "start_sample = int(start_time * beat_sr)\n",
    "end_sample = int(end_time * beat_sr)\n",
    "\n",
    "sliced_audio = beat_audio[start_sample:end_sample]\n",
    "sliced_clicks = audio_beat_clicks[start_sample:end_sample]\n",
    "\n",
    "ipd.display(ipd.Audio(sliced_audio + sliced_clicks, rate=beat_sr))\n",
    "selected_beat_frames = beat_frames[(beat_times >= start_time) & (beat_times <= end_time)]\n",
    "# plot_frequency_with_carnatic_notes(conf, selected_beat_frames, \"C#3\", beat_sr)\n",
    "\n",
    "\n",
    "shifted_beat_frames = shift_beats_to_peaks_or_valleys(beat_frames, conf)\n",
    "shifted_beat_times = librosa.frames_to_time(shifted_beat_frames, sr=beat_sr)\n",
    "\n",
    "audio_beat_clicks_shifted = librosa.clicks(times=shifted_beat_times, sr=beat_sr, click_freq=1500, length=len(beat_audio))\n",
    "\n",
    "# Play the sliced audio with the shifted clicks\n",
    "start_sample = int(start_time * beat_sr)\n",
    "end_sample = int(end_time * beat_sr)\n",
    "\n",
    "sliced_audio = beat_audio[start_sample:end_sample]\n",
    "sliced_clicks_shifted = audio_beat_clicks_shifted[start_sample:end_sample]\n",
    "\n",
    "ipd.display(ipd.Audio(sliced_audio + sliced_clicks_shifted, rate=beat_sr))\n",
    "\n",
    "# Plot frequency with the shifted beats\n",
    "selected_beat_frames_shifted = shifted_beat_frames[(shifted_beat_times >= start_time) & (shifted_beat_times <= end_time)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "brokenlist = breaklist(conf, shifted_beat_frames)\n",
    "brokenlist.pop(0)\n",
    "brokenlist.pop(-1)\n",
    "input_num = 83\n",
    "num = input_num - 1\n",
    "print(\"Playing segment\",input_num)\n",
    "\n",
    "\n",
    "\n",
    "play_segment_between_beats(beat_audio, beat_sr, shifted_beat_frames, num,offset=0)\n",
    "plot_with_carnatic_bars_with_peaks(num, brokenlist, get_carnatic_frequencies(\"C#3\"))\n",
    "\n",
    "def flatten_list(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "extrema_indices=sorted(flatten_list(find_peaks_and_valleys(brokenlist[num])))\n",
    "extrema_values = [brokenlist[num][i] for i in extrema_indices]\n",
    "potential_notes =[get_closest_note(i, get_carnatic_frequencies(\"C#3\")) for i in extrema_values]\n",
    "print(extrema_indices)\n",
    "print(potential_notes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_list = [interpolate_with_nans(i, target_length=128) for i in brokenlist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(conf))\n",
    "conf2 = conf[0:3000] # Assuming 'conf' is already defined and cleaned (silences as NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from fastdtw import fastdtw\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "\n",
    "def sliding_segments(conf, window_size, hop_size):\n",
    "    segments = []\n",
    "    indices = []\n",
    "    for i in range(0, len(conf) - window_size, hop_size):\n",
    "        segment = conf[i:i+window_size]\n",
    "        if np.isnan(segment).any():\n",
    "            continue  # Skip segments with silence\n",
    "        segments.append(segment)\n",
    "        indices.append(i)\n",
    "    return np.array(segments), np.array(indices)\n",
    "\n",
    "def non_overlapping_segments(conf, window_size, hop_size):\n",
    "    segments = []\n",
    "    indices = []\n",
    "    i = 0\n",
    "    while i < len(conf) - window_size:\n",
    "        segment = conf[i:i + window_size]\n",
    "        if np.isnan(segment).any():\n",
    "            i += hop_size\n",
    "            continue\n",
    "        segments.append(segment)\n",
    "        indices.append(i)\n",
    "        i += window_size  # skip all overlapping windows\n",
    "    return np.array(segments), np.array(indices)\n",
    "\n",
    "\n",
    "window_size = 60\n",
    "hop_size = int(window_size/12)\n",
    "\n",
    "segments, segment_starts = non_overlapping_segments(conf2, window_size, hop_size)\n",
    "# Center each segment to have zero mean (keep std deviation intact)\n",
    "# segments = np.array([seg - np.mean(seg) for seg in segments])\n",
    "\n",
    "\n",
    "print(f\"Extracted {len(segments)} valid segments.\")\n",
    "\n",
    "def dtw_distance_matrix(segments):\n",
    "    n = len(segments)\n",
    "    dists = np.zeros((n, n))\n",
    "    for i in tqdm(range(n)):\n",
    "        for j in range(i+1, n):\n",
    "            dist, _ = fastdtw(segments[i], segments[j])\n",
    "            dists[i, j] = dist\n",
    "            dists[j, i] = dist\n",
    "    return dists\n",
    "\n",
    "dtw_dists = dtw_distance_matrix(segments)\n",
    "\n",
    "# --- Step 3: Perform Clustering ---\n",
    "clustering = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    distance_threshold=70,\n",
    "    metric='precomputed',\n",
    "    linkage='average'\n",
    ")\n",
    "labels = clustering.fit_predict(dtw_dists)\n",
    "print(f\"Found {len(set(labels))} clusters.\")\n",
    "\n",
    "# --- Step 4: Group segments by cluster ---\n",
    "cluster_dict = defaultdict(list)\n",
    "cluster_origins = defaultdict(list)\n",
    "\n",
    "for seg, start_idx, lbl in zip(segments, segment_starts, labels):\n",
    "    cluster_dict[lbl].append(seg)\n",
    "    cluster_origins[lbl].append(start_idx)\n",
    "\n",
    "\n",
    "# --- Step 5: Plot number of segments per cluster ---\n",
    "cluster_sizes = {label: len(segments) for label, segments in cluster_dict.items()}\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Adjust height dynamically but cap it\n",
    "fig_height = min(20, len(cluster_sizes) * 0.4)  # max 20 inches tall\n",
    "plt.figure(figsize=(12, fig_height))\n",
    "\n",
    "# Sort cluster labels\n",
    "sorted_labels = sorted(cluster_sizes.keys())\n",
    "sorted_counts = [cluster_sizes[label] for label in sorted_labels]\n",
    "\n",
    "bars = plt.barh(sorted_labels, sorted_counts, color='lightgreen')\n",
    "\n",
    "plt.ylabel('Cluster Label')\n",
    "plt.xlabel('Number of Segments')\n",
    "plt.title('Segment Counts per Cluster')\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add count labels on each bar\n",
    "for bar, count in zip(bars, sorted_counts):\n",
    "    plt.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
    "             f'{count}', va='center', fontsize=9)\n",
    "\n",
    "# Ensure all labels are shown\n",
    "plt.yticks(sorted_labels)  # Force each cluster label to appear\n",
    "plt.gca().yaxis.set_major_locator(ticker.FixedLocator(sorted_labels))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clusters = sorted(cluster_dict.items(), key=lambda item: len(item[1]), reverse=True)\n",
    "for i in range(len(sorted_clusters)):\n",
    "    if len(sorted_clusters[i][1])== len(sorted_clusters[0][1]):\n",
    "        print(f\"the cluster {sorted_clusters[i][0]} has {len(sorted_clusters[i][1])} segments\")\n",
    "\n",
    "clustersize=[]\n",
    "clusterlabels=[]\n",
    "\n",
    "for i in range(len(sorted_clusters)):\n",
    "    clustersize.append(len(sorted_clusters[i][1]))\n",
    "    clusterlabels.append(sorted_clusters[i][0])\n",
    "print(clusterlabels)\n",
    "\n",
    "print(np.std(clustersize))\n",
    "print(np.mean(clustersize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = sorted_clusters[0][0]\n",
    "label_to_plot = label # Change this as needed\n",
    "segment_list = cluster_dict[label_to_plot]\n",
    "\n",
    "for segment in segment_list:\n",
    "    print(cluster_origins[label_to_plot])\n",
    "    plt.plot(segment, alpha=0.5)\n",
    "plt.title(f\"Cluster {label_to_plot} - {len(segment_list)} Segments\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_with_carnatic_bars_with_peaks(0, segment_list, get_carnatic_frequencies(\"C#3\"))\n",
    "\n",
    "\n",
    "\n",
    "start = cluster_origins[label_to_plot][0]\n",
    "print(start)\n",
    "play_segment(beat_audio, beat_sr, start, start+window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_cluster = sorted_clusters[0][0]\n",
    "start_indices = cluster_origins[chosen_cluster]\n",
    "origins=[]\n",
    "for i in start_indices:\n",
    "    origins.append((i,i+window_size))\n",
    "start_indices = [int(i) for i in cluster_origins[chosen_cluster]]\n",
    "print(start_indices)\n",
    "\n",
    "for i in start_indices:\n",
    "    conf2[i:i+30] = np.nan\n",
    "plt.plot(conf2, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf2 = conf\n",
    "\n",
    "def extract_notes_from_conf(conf, initial_window_size, decay_size, min_window_size, outlier_threshold,similairity_threshold=100):\n",
    "    conf = conf.copy()\n",
    "    remaining_conf = conf.copy()\n",
    "    all_removed_segments = []\n",
    "\n",
    "    window_size = initial_window_size\n",
    "    global_label_offset = 0\n",
    "\n",
    "    total_iters = (initial_window_size - min_window_size) // decay_size + 1\n",
    "    iter_count = 0\n",
    "\n",
    "    while window_size >= min_window_size:\n",
    "        iter_count += 1\n",
    "        print(f\"Iteration {iter_count}/{total_iters} — Window Size: {window_size}\")\n",
    "\n",
    "        hop_size = int(window_size / 12)\n",
    "        segments, segment_starts = non_overlapping_segments(remaining_conf, window_size, hop_size)\n",
    "        \n",
    "        if len(segments) == 0:\n",
    "            print(\"  Skipped — no valid segments\")\n",
    "            window_size -= decay_size\n",
    "            continue\n",
    "\n",
    "        dtw_dists = dtw_distance_matrix(segments)\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=similairity_threshold,\n",
    "            metric='precomputed',\n",
    "            linkage='average'\n",
    "        )\n",
    "        labels = clustering.fit_predict(dtw_dists)\n",
    "\n",
    "        cluster_dict = defaultdict(list)\n",
    "        cluster_origins = defaultdict(list)\n",
    "\n",
    "        for seg, start_idx, lbl in zip(segments, segment_starts, labels):\n",
    "            cluster_dict[lbl].append(seg)\n",
    "            cluster_origins[lbl].append(start_idx)\n",
    "\n",
    "        clustered = False\n",
    "        for label, starts in cluster_origins.items():\n",
    "            if len(starts) >= outlier_threshold:\n",
    "                clustered = True\n",
    "                global_label = global_label_offset + label\n",
    "                for i in starts:\n",
    "                    remaining_conf[i:i + window_size] = np.nan\n",
    "                    all_removed_segments.append((i, i + window_size, global_label))\n",
    "\n",
    "        if clustered:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, removed some segments.\")\n",
    "        else:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, but none met the threshold.\")\n",
    "\n",
    "        global_label_offset += len(set(labels))\n",
    "        window_size -= decay_size\n",
    "\n",
    "    return remaining_conf, all_removed_segments\n",
    "\n",
    "\n",
    "def plot_colored_segments(original_conf, removed_segments, residual_conf=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # Assign a color to each cluster\n",
    "    cluster_labels = sorted(set(lbl for _, _, lbl in removed_segments))\n",
    "    cmap = cm.get_cmap('tab20', len(cluster_labels))\n",
    "    cluster_to_color = {label: cmap(i) for i, label in enumerate(cluster_labels)}\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(original_conf, label=\"Original\", alpha=0.2, color='gray')\n",
    "\n",
    "    # Plot segments grouped by cluster label\n",
    "    for start, end, label in removed_segments:\n",
    "        plt.plot(range(start, end), original_conf[start:end], color=cluster_to_color[label], label=f\"Cluster {label}\")\n",
    "\n",
    "    # Plot residual if given\n",
    "    if residual_conf is not None:\n",
    "        plt.plot(residual_conf, label=\"Residual\", linewidth=2, color='black')\n",
    "\n",
    "    # Create a legend without duplicate labels\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.title(\"Clusters of Repeating Notes\")\n",
    "    plt.xlabel(\"Time Index\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "residual_conf, removed_segments = extract_notes_from_conf(\n",
    "    conf=conf2,\n",
    "    initial_window_size=60,\n",
    "    decay_size=2,\n",
    "    min_window_size=20,\n",
    "    outlier_threshold=2,\n",
    "    similairity_threshold=80\n",
    ")\n",
    "\n",
    "plot_colored_segments(conf2, removed_segments, residual_conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1 =239\n",
    "num2 =num1+1 \n",
    "num3= num2+1\n",
    "unique_cluster_labels = set(label for _, _, label in removed_segments)\n",
    "print(\"Number of clusters:\", len(unique_cluster_labels))\n",
    "\n",
    "print(\"len\",len(removed_segments))\n",
    "print(removed_segments[num1])\n",
    "print(removed_segments[num2])\n",
    "print(removed_segments[num3])\n",
    "\n",
    "play_segment(beat_audio, beat_sr, removed_segments[num1][0], removed_segments[num1][1])\n",
    "play_segment(beat_audio, beat_sr, removed_segments[num2][0], removed_segments[num2][1])\n",
    "play_segment(beat_audio, beat_sr, removed_segments[num3][0], removed_segments[num3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np # Make sure numpy is imported if not already\n",
    "\n",
    "def save_removed_segments_to_json(removed_segments, filename=\"removed_segments.json\"):\n",
    "    \"\"\"\n",
    "    Saves the list of removed segments directly to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        removed_segments (list): The list of removed segments (e.g., a list of tuples).\n",
    "        filename (str, optional): The name of the JSON file to save to.\n",
    "                                   Defaults to \"removed_segments.json\".\n",
    "    \"\"\"\n",
    "    # Convert any non-serializable elements (like numpy.int64) to standard Python types\n",
    "    serializable_segments = []\n",
    "    for segment in removed_segments:\n",
    "        serializable_segment = [int(item) for item in segment]\n",
    "        serializable_segments.append(serializable_segment)\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(serializable_segments, f, indent=4)\n",
    "    print(f\"Removed segments list saved to '{filename}'\")\n",
    "\n",
    "save_removed_segments_to_json(removed_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def recover_removed_segments_list(filename=\"removed_segments.json\"):\n",
    "    \"\"\"\n",
    "    Recovers the list of removed segments from a JSON file saved in the\n",
    "    simple list format.\n",
    "\n",
    "    Args:\n",
    "        filename (str, optional): The name of the JSON file to load from.\n",
    "                                   Defaults to \"removed_segments.json\".\n",
    "\n",
    "    Returns:\n",
    "        list or None: The list of removed segments, or None if the file\n",
    "                      is not found or doesn't contain a top-level list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"Error: File '{filename}' does not contain a top-level list.\")\n",
    "                return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from file '{filename}'.\")\n",
    "        return None\n",
    "\n",
    "normalized_notes=[]\n",
    "labels=[]\n",
    "# Example of how to recover the list:\n",
    "recovered_list = recover_removed_segments_list()\n",
    "if recovered_list:\n",
    "    print(\"Recovered removed segments list:\")\n",
    "    for segment in recovered_list:\n",
    "        note=conf[segment[0]:segment[1]]/get_carnatic_frequencies(\"C#3\")[\"Sa\"]\n",
    "        normalized_notes.append(note)\n",
    "        labels.append(segment[2])\n",
    "print(normalized_notes[2][4])\n",
    "\n",
    "\n",
    "print(audio_path)\n",
    "print(len(normalized_notes))\n",
    "print(len(labels))\n",
    "print(len(recovered_list))\n",
    "print(get_carnatic_frequencies(\"C#3\")[\"Sa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "csv_file = \"Master_Dataset.csv\"\n",
    "headers = ['No','File','Tonic','Normalized_Notes','Start_End','Cluster','Notes','Gamaka']\n",
    "\n",
    "rows_to_append = []\n",
    "for i in range(len(recovered_list)):\n",
    "    rows_to_append.append([\n",
    "        i,  # Assuming you want to start from 1\n",
    "        audio_path,\n",
    "        \"C#3\",\n",
    "        # dump the list as a JSON string\n",
    "        json.dumps(list(normalized_notes[i])),\n",
    "        json.dumps(list(recovered_list[i][0:2])),\n",
    "        labels[i],\n",
    "        \"\",\n",
    "        \"\"\n",
    "    ])\n",
    "\n",
    "with open(csv_file, mode='a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows_to_append:\n",
    "        if len(row) != len(headers):\n",
    "            raise ValueError(f\"Row has {len(row)} columns but expected {len(headers)}\")\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "csv_file = \"Master_Dataset.csv\"\n",
    "\n",
    "norm_notes = []\n",
    "start_end = []\n",
    "\n",
    "with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for r in reader:\n",
    "        norm_notes.append(json.loads(r['Normalized_Notes']))\n",
    "        start_end.append(json.loads(r['Start_End']))\n",
    "\n",
    "print(start_end)       # This will now show the full list of all start_end entries\n",
    "print(norm_notes)      # Same for Normalized_Notes\n",
    "\n",
    "for i in range(len(start_end)):\n",
    "    print(i+2)\n",
    "    play_segment(beat_audio, beat_sr, int(start_end[i][0]), int(start_end[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpnotes=[]\n",
    "for i in norm_notes:\n",
    "    interpnotes+=interpolate_with_nans(i, target_length=128).tolist()\n",
    "print(len(interpnotes))\n",
    "\n",
    "plt.plot(interpnotes, alpha=0.5)\n",
    "plt.title(\"Interpolated Notes\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf2 = interpnotes\n",
    "\n",
    "def extract_notes_from_conf(conf, initial_window_size, decay_size, min_window_size, outlier_threshold,similairity_threshold=100):\n",
    "    conf = conf.copy()\n",
    "    remaining_conf = conf.copy()\n",
    "    all_removed_segments = []\n",
    "\n",
    "    window_size = initial_window_size\n",
    "    global_label_offset = 0\n",
    "\n",
    "    total_iters = (initial_window_size - min_window_size) // decay_size + 1\n",
    "    iter_count = 0\n",
    "\n",
    "    while window_size >= min_window_size:\n",
    "        iter_count += 1\n",
    "        print(f\"Iteration {iter_count}/{total_iters} — Window Size: {window_size}\")\n",
    "\n",
    "        hop_size = int(window_size / 12)\n",
    "        segments, segment_starts = non_overlapping_segments(remaining_conf, window_size, hop_size)\n",
    "        \n",
    "        if len(segments) == 0:\n",
    "            print(\"  Skipped — no valid segments\")\n",
    "            window_size -= decay_size\n",
    "            continue\n",
    "\n",
    "        dtw_dists = dtw_distance_matrix(segments)\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=similairity_threshold,\n",
    "            metric='precomputed',\n",
    "            linkage='average'\n",
    "        )\n",
    "        labels = clustering.fit_predict(dtw_dists)\n",
    "\n",
    "        cluster_dict = defaultdict(list)\n",
    "        cluster_origins = defaultdict(list)\n",
    "\n",
    "        for seg, start_idx, lbl in zip(segments, segment_starts, labels):\n",
    "            cluster_dict[lbl].append(seg)\n",
    "            cluster_origins[lbl].append(start_idx)\n",
    "\n",
    "        clustered = False\n",
    "        for label, starts in cluster_origins.items():\n",
    "            if len(starts) >= outlier_threshold:\n",
    "                clustered = True\n",
    "                global_label = global_label_offset + label\n",
    "                for i in starts:\n",
    "                    remaining_conf[i:i + window_size] = np.nan\n",
    "                    all_removed_segments.append((i, i + window_size, global_label))\n",
    "\n",
    "        if clustered:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, removed some segments.\")\n",
    "        else:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, but none met the threshold.\")\n",
    "\n",
    "        global_label_offset += len(set(labels))\n",
    "        window_size -= decay_size\n",
    "\n",
    "    return remaining_conf, all_removed_segments\n",
    "\n",
    "\n",
    "def plot_colored_segments(original_conf, removed_segments, residual_conf=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # Assign a color to each cluster\n",
    "    cluster_labels = sorted(set(lbl for _, _, lbl in removed_segments))\n",
    "    cmap = cm.get_cmap('tab20', len(cluster_labels))\n",
    "    cluster_to_color = {label: cmap(i) for i, label in enumerate(cluster_labels)}\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(original_conf, label=\"Original\", alpha=0.2, color='gray')\n",
    "\n",
    "    # Plot segments grouped by cluster label\n",
    "    for start, end, label in removed_segments:\n",
    "        plt.plot(range(start, end), original_conf[start:end], color=cluster_to_color[label], label=f\"Cluster {label}\")\n",
    "\n",
    "    # Plot residual if given\n",
    "    if residual_conf is not None:\n",
    "        plt.plot(residual_conf, label=\"Residual\", linewidth=2, color='black')\n",
    "\n",
    "    # Create a legend without duplicate labels\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.title(\"Clusters of Repeating Notes\")\n",
    "    plt.xlabel(\"Time Index\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "residual_conf, removed_segments = extract_notes_from_conf(\n",
    "    conf=conf2,\n",
    "    initial_window_size=60,\n",
    "    decay_size=2,\n",
    "    min_window_size=20,\n",
    "    outlier_threshold=2,\n",
    "    similairity_threshold=80\n",
    ")\n",
    "\n",
    "plot_colored_segments(conf2, removed_segments, residual_conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
