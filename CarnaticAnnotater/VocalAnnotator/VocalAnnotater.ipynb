{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bf10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.\\.venv\\Scripts\\Activate.ps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0179641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import crepe\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import argrelextrema\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import IPython.display as ipd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from scipy.signal import find_peaks as scipy_find_peaks\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c623867",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_raaga(\"Mayamalavagowlai\", \"Mayamalavagowlai_Vocals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acaab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import crepe\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# === SETTINGS ===\n",
    "vocal_folder = \"CarnaticAnnotater\\Mayamalavagowlai_Vocals\"  # Your folder with WAV files\n",
    "output_csv = \"CarnaticAnnotater\\Master_Crepe_Mayamalavagowlai.csv\"\n",
    "log_csv = \"CarnaticAnnotater\\VocalAnnotator\\Crepe_logMayamalavagowlai.csv\"\n",
    "\n",
    "# === ENSURE LOG FILE EXISTS AND HAS HEADER ===\n",
    "if not os.path.exists(log_csv) or os.path.getsize(log_csv) == 0:\n",
    "    # Create it with correct header\n",
    "    with open(log_csv, \"w\") as f:\n",
    "        f.write(\"AudioPath\\n\")\n",
    "    processed_files = set()\n",
    "else:\n",
    "    processed_files = set(pd.read_csv(log_csv)[\"AudioPath\"].values)\n",
    "\n",
    "# === ENSURE MASTER CSV EXISTS AND HAS HEADER ===\n",
    "if not os.path.exists(output_csv) or os.path.getsize(output_csv) == 0:\n",
    "    with open(output_csv, \"w\") as f:\n",
    "        f.write(\"Index,AudioPath,Raaga,SongName,Tonic,Time,Frequency,Confidence\\n\")\n",
    "    current_song_index = 1 # Initialize for the first song\n",
    "else:\n",
    "    df_existing = pd.read_csv(output_csv)\n",
    "    # Get the max song index if the file is not empty and has an 'Index' column\n",
    "    if not df_existing.empty and 'Index' in df_existing.columns:\n",
    "        current_song_index = df_existing[\"Index\"].max() + 1\n",
    "    else:\n",
    "        current_song_index = 1 # Start from 1 if no existing data or 'Index' column\n",
    "\n",
    "new_log_entries = []\n",
    "\n",
    "# === PROCESS WAV FILES ===\n",
    "for filename in os.listdir(vocal_folder):\n",
    "    if not filename.lower().endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    audio_path = os.path.join(vocal_folder, filename)\n",
    "\n",
    "    if audio_path in processed_files:\n",
    "        print(f\"🟡 Skipping already processed file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"🔧 Processing new file: {filename}\")\n",
    "\n",
    "    # ✅ Extract Raaga, SongName, Tonic\n",
    "    name_without_ext = filename.replace(\".wav\", \"\")\n",
    "    parts = name_without_ext.split(\"_\")\n",
    "\n",
    "    if len(parts) >= 3:\n",
    "        raaga = parts[0]\n",
    "        songname = \"_\".join(parts[1:-1])\n",
    "        tonic = parts[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Filename not in expected format: {filename}\")\n",
    "\n",
    "    # === Load audio ===\n",
    "    y, sr = librosa.load(audio_path, sr=44100)\n",
    "\n",
    "    # === CREPE pitch estimation ===\n",
    "    time, frequency, confidence, _ = crepe.predict(y, sr, viterbi=True, step_size=20, model_capacity=\"tiny\")\n",
    "\n",
    "    # === Interpolate to STFT time grid ===\n",
    "    spec_time = librosa.times_like(librosa.stft(y), sr=sr)\n",
    "    interp_freq = interp1d(time, frequency, kind='linear', fill_value='extrapolate')\n",
    "    interp_conf = interp1d(time, confidence, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "    new_frequency = interp_freq(spec_time)\n",
    "    new_confidence = interp_conf(spec_time)\n",
    "\n",
    "    # ✅ Remove silent parts by confidence threshold\n",
    "    mask = new_confidence > 0.7  # Adjust threshold as needed\n",
    "    spec_time = spec_time[mask]\n",
    "    new_frequency = new_frequency[mask]\n",
    "    new_confidence = new_confidence[mask]\n",
    "\n",
    "    # === Assign the same song index to all rows of the current song ===\n",
    "    df_single = pd.DataFrame({\n",
    "        \"Index\": [current_song_index]*len(spec_time), # This is the key change\n",
    "        \"AudioPath\": [audio_path]*len(spec_time),\n",
    "        \"Raaga\": [raaga]*len(spec_time),\n",
    "        \"SongName\": [songname]*len(spec_time),\n",
    "        \"Tonic\": [tonic]*len(spec_time),\n",
    "        \"Time\": spec_time,\n",
    "        \"Frequency\": new_frequency,\n",
    "        \"Confidence\": new_confidence\n",
    "    })\n",
    "\n",
    "    df_single.to_csv(output_csv, mode='a', index=False, header=False)\n",
    "    new_log_entries.append(audio_path)\n",
    "    current_song_index += 1 # Increment for the next song\n",
    "    print(f\"✅ Done with: {filename}\")\n",
    "\n",
    "# === UPDATE LOG ===\n",
    "if new_log_entries:\n",
    "    log_df = pd.DataFrame({\"AudioPath\": new_log_entries})\n",
    "    log_df.to_csv(log_csv, mode='a', index=False, header=False)\n",
    "\n",
    "print(\"🎉 All new files processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram_with_crepe(spec_time, conf, S_db, sr):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='linear', cmap='viridis')\n",
    "    plt.plot(spec_time, conf, color='r', linewidth=1.5, label='CREPE Pitch')  # Use spec_time\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.ylim(0, 2000)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def find_tonic(S, sr):\n",
    "    chroma = librosa.feature.chroma_stft(S=np.abs(S), sr=sr)\n",
    "    pitch_class_sums = np.sum(np.abs(chroma), axis=1)\n",
    "    pitch_labels = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    pitch_class_dict = dict(zip(pitch_labels, pitch_class_sums))\n",
    "    return max(pitch_class_dict, key=pitch_class_dict.get)\n",
    "\n",
    "def get_carnatic_frequencies(tonic):\n",
    "    # Intonational ratios for the basic set of Carnatic notes\n",
    "    carnatic_ratios = {\n",
    "        'sa': 0.5*1.0,    # Tonic (Sa)\n",
    "        'ri1': 0.5*16/15, # Ri1\n",
    "        'ri2': 0.5*9/8,  # Ri2\n",
    "        'ga1': 0.5*6/5,  # Ga1\n",
    "        'ga2': 0.5*5/4, # Ga2\n",
    "        'ma1': 0.5*4/3, # Ma1\n",
    "        'ma2': 0.5*45/32,   # Ma2\n",
    "        'pa': 0.5*3/2,    # Pa\n",
    "        'da1': 0.5*8/5, # Dha1\n",
    "        'da2': 0.5*5/3, # Dha2\n",
    "        'ni1': 0.5*16/9, # Ni1\n",
    "        'ni2': 0.5*15/8,   # Ni2\n",
    "\n",
    "        'Sa': 1.0,    # Tonic (Sa)\n",
    "        'Ri1': 16/15, # Ri1\n",
    "        'Ri2': 9/8,  # Ri2\n",
    "        'Ga1': 6/5,  # Ga1\n",
    "        'Ga2': 5/4, # Ga2\n",
    "        'Ma1': 4/3, # Ma1\n",
    "        'Ma2': 45/32,   # Ma2\n",
    "        'Pa': 3/2,    # Pa\n",
    "        'Da1': 8/5, # Dha1\n",
    "        'Da2': 5/3, # Dha2\n",
    "        'Ni1': 16/9, # Ni1\n",
    "        'Ni2': 15/8,   # Ni2\n",
    "\n",
    "        'SA': 2.0,   # Octave higher (Sa)\n",
    "        'RI1': 2*16/15, # Ri1\n",
    "        'RI2': 2*9/8,  # Ri2\n",
    "        'GA1': 2*6/5,  # Ga1\n",
    "        'GA2': 2*5/4, # Ga2\n",
    "        'MA1': 2*4/3, # Ma1\n",
    "        'MA2': 2*45/32,   # Ma2\n",
    "        'PA': 2*3/2,    # Pa\n",
    "        'DA1': 2*8/5, # Dha1\n",
    "        'DA2': 2*5/3, # Dha2\n",
    "        'NI1': 2*16/9, # Ni1\n",
    "        'NI2': 2*15/8,   \n",
    "    }\n",
    "\n",
    "    tonic_freq = librosa.note_to_hz(tonic)  # Get the frequency of the tonic\n",
    "\n",
    "    # Calculate the frequencies for each Carnatic note relative to the tonic\n",
    "    carnatic_frequencies = {note: tonic_freq * ratio for note, ratio in carnatic_ratios.items()}\n",
    "    return carnatic_frequencies\n",
    "\n",
    "def get_closest_note(freq, carnatic_frequencies):\n",
    "    \"\"\"Find the closest Carnatic note for a given frequency.\"\"\"\n",
    "    return min(carnatic_frequencies, key=lambda note: abs(carnatic_frequencies[note] - freq))\n",
    "\n",
    "def get_closest_frequency(freq, carnatic_frequencies):\n",
    "    \"\"\"Find the closest Carnatic note frequency for a given frequency.\"\"\"\n",
    "    return min(carnatic_frequencies.values(), key=lambda f: abs(f - freq))\n",
    "\n",
    "def get_index_from_time(time_input,conf):\n",
    "    # Define the start and end times\n",
    "    total_duration = end_time - start_time\n",
    "    num_pieces = len(conf)\n",
    "    \n",
    "    # Calculate the duration of each piece\n",
    "    duration_per_piece = total_duration / num_pieces\n",
    "    \n",
    "    # Check if the input time is within the valid range\n",
    "    if time_input < start_time or time_input > end_time:\n",
    "        raise ValueError(f\"Input time must be between {start_time} and {end_time} seconds.\")\n",
    "    \n",
    "    # Calculate the index\n",
    "    index = int((time_input - start_time) / duration_per_piece)\n",
    "    \n",
    "    return index\n",
    "\n",
    "def plot_frequency_with_carnatic_notes(frequency_list, beat_frames, tonic,beat_sr):\n",
    "    beat_frames= librosa.frames_to_time(beat_frames, sr=beat_sr)\n",
    "    loc_extremes = np.where(np.diff(np.sign(np.diff(frequency_list, prepend=np.nan, append=np.nan))) != 0)[0]\n",
    "    extremes = frequency_list[loc_extremes].tolist()\n",
    "    angles = np.degrees(np.arctan(np.diff(frequency_list, prepend=np.nan, append=np.nan) / 2))\n",
    "    # notelist = [(conf[i], i, angles[i], angles[i + 1], i in loc_extremes) for i in range(len(conf) - 1)]\n",
    "    carnatic_frequencies = get_carnatic_frequencies(tonic)\n",
    "    frequency_array = np.array(frequency_list)\n",
    "    \n",
    "    beat_points=[]\n",
    "    for i in beat_frames:\n",
    "        if i < start_time or i > end_time:\n",
    "            continue\n",
    "        beat_points.append(get_index_from_time(i,frequency_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Identify valid (non-NaN) frames\n",
    "    valid_indices = ~np.isnan(frequency_array)  \n",
    "    valid_frequencies = frequency_array[valid_indices]\n",
    "    if len(valid_frequencies) == 0:\n",
    "        raise ValueError(\"No valid frequencies to process.\")\n",
    "\n",
    "    carnatic_frequencies = get_carnatic_frequencies(tonic)\n",
    "\n",
    "    # Plot the graph\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot the frequency graph with gaps for NaNs\n",
    "    for start, end in zip(\n",
    "        np.where(np.diff(np.concatenate(([0], valid_indices, [0]))) == 1)[0],\n",
    "        np.where(np.diff(np.concatenate(([0], valid_indices, [0]))) == -1)[0]\n",
    "    ):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.arange(start, end),\n",
    "            y=frequency_array[start:end],\n",
    "            mode='lines',\n",
    "            name='Frequency (Hz)',\n",
    "            line=dict(color='blue')\n",
    "        ))\n",
    "\n",
    "    # Plot horizontal lines for Carnatic notes\n",
    "    for note, freq in carnatic_frequencies.items():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[0, len(frequency_list) - 1],\n",
    "            y=[freq, freq],\n",
    "            mode='lines',\n",
    "            line=dict(dash='dash', color='gray', width=2),\n",
    "            name=note,\n",
    "            hovertemplate=f\"{note} ({freq:.2f} Hz)\"\n",
    "        ))\n",
    "\n",
    "    # Plot the extremes as red dots\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=loc_extremes,\n",
    "        y=extremes,\n",
    "        mode='markers',\n",
    "        marker=dict(color='red', size=2, symbol='circle'),\n",
    "        name='Extremes'\n",
    "    ))\n",
    "\n",
    "    # Plot vertical lines for beat points\n",
    "    for beat in beat_points:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[beat, beat],  # Vertical line at 'beat'\n",
    "            y=[np.nanmin(frequency_array), np.nanmax(frequency_array)],  # Full y-range\n",
    "            mode='lines',\n",
    "            line=dict(color='orange', width=2),\n",
    "            name=f'Beat @ {beat}'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Frequency with Carnatic Notes (Tonic: {tonic})',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Frequency (Hz)',\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def breaklist(elements, indexes):\n",
    "    segmented_lists = []\n",
    "    start_index = 0  \n",
    "\n",
    "    for idx in indexes:\n",
    "        segment = elements[start_index:idx]\n",
    "        segmented_lists.append(segment)\n",
    "        start_index = idx  \n",
    "    if start_index < len(elements):\n",
    "        segmented_lists.append(elements[start_index:])\n",
    "\n",
    "    return segmented_lists\n",
    "\n",
    "def plot_with_carnatic_bars(note_num, noteslist, carnatic_frequencies):\n",
    "    bars = list(carnatic_frequencies.values())\n",
    "    \n",
    "    # Find relevant frequency range\n",
    "    min_freq = get_closest_frequency(np.nanmin(noteslist[note_num]), carnatic_frequencies)\n",
    "    max_freq = get_closest_frequency(np.nanmax(noteslist[note_num]), carnatic_frequencies)\n",
    "    \n",
    "    # Filter bars within the frequency range\n",
    "    newbars = [i for i in bars if min_freq <= i <= max_freq]\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(noteslist[note_num])\n",
    "    for i in newbars:\n",
    "        plt.axhline(y=i, color='r', linestyle='--')\n",
    "    plt.show()\n",
    "    for i in newbars:\n",
    "        print(get_closest_note(i, carnatic_frequencies))\n",
    "\n",
    "def spectral_decomp(note, n_clusters, plot=True):\n",
    "    note = np.array(note)\n",
    "    X = np.column_stack((np.arange(len(note)), note))\n",
    "    embedding = SpectralEmbedding(n_components=2, affinity='nearest_neighbors')\n",
    "    X_transformed = embedding.fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_transformed)\n",
    "\n",
    "    # Sort clusters based on first occurrence\n",
    "    unique_clusters = np.unique(labels, return_index=True)\n",
    "    sorted_clusters = [cluster for _, cluster in sorted(zip(unique_clusters[1], unique_clusters[0]))]\n",
    "    label_mapping = {old: new for new, old in enumerate(sorted_clusters)}\n",
    "    sorted_labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "    # Assign frequencies to clusters\n",
    "    segments = [[] for _ in range(n_clusters)]\n",
    "    for idx, freq in enumerate(note):\n",
    "        segments[sorted_labels[idx]].append((idx, freq))\n",
    "\n",
    "    if plot:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan', 'magenta']\n",
    "        for i in range(n_clusters):\n",
    "            indices, freqs = zip(*segments[i]) if segments[i] else ([], [])\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=indices,\n",
    "                y=freqs,\n",
    "                mode='markers+lines',\n",
    "                marker=dict(size=6, color=colors[i % len(colors)]),\n",
    "            ))\n",
    "\n",
    "        # Plot horizontal lines at each unique frequency\n",
    "\n",
    "        unique_freqs = [i for i in get_carnatic_frequencies(\"C#3\").values() if min(note) <= i <= max(note)]\n",
    "        unique_notes= [i for i in get_carnatic_frequencies(\"C#3\").keys() if min(note) <= get_carnatic_frequencies(\"C#3\")[i] <= max(note)]\n",
    "        x_values = np.linspace(min(X[:, 0]), max(X[:, 0]), num=100)  # Densely spaced x values\n",
    "\n",
    "        for i in range (len( unique_freqs)):\n",
    "            y_values = np.full_like(x_values,unique_freqs[i])\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_values,\n",
    "                y=y_values,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"gray\", dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "                hovertemplate=f\"{unique_notes[i]}({unique_freqs[i]:.2f} Hz)\"\n",
    "            ))\n",
    "\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "    return [list(zip(*seg))[1] if seg else [] for seg in segments] \n",
    "\n",
    "def playnote(n, beat_audio, beat_sr, beat_times, start_time):\n",
    "    adjusted_beat_times = beat_times - start_time\n",
    "    adjusted_beat_times = adjusted_beat_times[adjusted_beat_times >= 0]  # Remove negative times\n",
    "    if n < 0 or n >= len(adjusted_beat_times) - 1:\n",
    "        print(\"Invalid note index\")\n",
    "        return\n",
    "    note_start_time = adjusted_beat_times[n]\n",
    "    note_end_time = adjusted_beat_times[n+1]\n",
    "    start_sample = int(note_start_time * beat_sr)\n",
    "    end_sample = int(note_end_time * beat_sr)\n",
    "\n",
    "    note_audio = beat_audio[start_sample:end_sample]\n",
    "    ipd.display(ipd.Audio(note_audio, rate=beat_sr))\n",
    "\n",
    "def find_peaks_and_valleys(conf):\n",
    "    peaks = []\n",
    "    valleys = []\n",
    "    \n",
    "    for i in range(1, len(conf) - 1):\n",
    "        if not np.isnan(conf[i-1]) and not np.isnan(conf[i]) and not np.isnan(conf[i+1]):\n",
    "            if conf[i] > conf[i-1] and conf[i] > conf[i+1]:\n",
    "                peaks.append(i)\n",
    "            elif conf[i] < conf[i-1] and conf[i] < conf[i+1]:\n",
    "                valleys.append(i)\n",
    "    \n",
    "    return peaks, valleys\n",
    "\n",
    "def play_segment_between_beats(beat_audio, beat_sr, beat_frames, beat_index,offset=0):\n",
    "    # Ensure the beat_index is valid\n",
    "    if beat_index < 0 or beat_index >= len(beat_frames) - 1:\n",
    "        print(\"Invalid beat index. Please provide a valid index.\")\n",
    "        return\n",
    "\n",
    "    # Get the start and end frames for the segment\n",
    "    start_frame = beat_frames[beat_index-offset]\n",
    "    end_frame = beat_frames[beat_index + 1+offset]\n",
    "\n",
    "    # Convert frames to time\n",
    "    start_time = librosa.frames_to_time(start_frame, sr=beat_sr)\n",
    "    end_time = librosa.frames_to_time(end_frame, sr=beat_sr)\n",
    "\n",
    "    # Convert time to sample indices\n",
    "    start_sample = int(start_time * beat_sr)\n",
    "    end_sample = int(end_time * beat_sr)\n",
    "\n",
    "    # Slice the audio segment\n",
    "    audio_segment = beat_audio[start_sample:end_sample]\n",
    "\n",
    "    # Play the audio segment\n",
    "    ipd.display(ipd.Audio(audio_segment, rate=beat_sr))\n",
    "\n",
    "def trim(data):\n",
    "    data = np.array(data)  \n",
    "    valid_indices = np.where(~np.isnan(data))[0]\n",
    "    valid_data = data[valid_indices]\n",
    "    peaks = argrelextrema(valid_data, np.greater, order=2)[0]\n",
    "\n",
    "    troughs = argrelextrema(valid_data, np.less, order=2)[0]\n",
    "\n",
    "    # Combine peaks & troughs and sort them\n",
    "    extrema = np.sort(np.concatenate((peaks, troughs)))\n",
    "\n",
    "    if len(extrema) < 2:\n",
    "        return data  # Not enough peaks/troughs to trim\n",
    "\n",
    "    # Find start and end positions in original indices\n",
    "    start, end = valid_indices[extrema[0]], valid_indices[extrema[-1]]\n",
    "\n",
    "    return data[start:end+1]\n",
    "\n",
    "def shift_beats_to_peaks_or_valleys(beat_frames, conf):\n",
    "    \"\"\"\n",
    "    Shift the beat frames to align with the nearest peak or valley in the confidence array.\n",
    "    \n",
    "    Parameters:\n",
    "    - beat_frames: The original beat frames.\n",
    "    - conf: The confidence array.\n",
    "    \n",
    "    Returns:\n",
    "    - shifted_beat_frames: The updated beat frames.\n",
    "    \"\"\"\n",
    "    peaks, valleys = find_peaks_and_valleys(conf)\n",
    "    shifted_beat_frames = []\n",
    "\n",
    "    for beat in beat_frames:\n",
    "        # Find the nearest peak or valley\n",
    "        nearest_index = None\n",
    "        min_distance = float('inf')\n",
    "\n",
    "        for index in peaks + valleys:\n",
    "            distance = abs(index - beat)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_index = index\n",
    "\n",
    "        shifted_beat_frames.append(nearest_index)\n",
    "\n",
    "    return np.array(shifted_beat_frames)\n",
    "\n",
    "def extend_sublists(main_list, num=4):\n",
    "    extended_list = []\n",
    "    for i in range(len(main_list)):\n",
    "        current_sublist = main_list[i]\n",
    "        if i == 0 or i == len(main_list) - 1:\n",
    "            extended_list.append(current_sublist)\n",
    "        else:\n",
    "            new_sublist = []\n",
    "            new_sublist.extend(main_list[i - 1][-num:])\n",
    "            new_sublist.extend(current_sublist)\n",
    "            new_sublist.extend(main_list[i + 1][:num])\n",
    "            extended_list.append(new_sublist)\n",
    "    return extended_list\n",
    "\n",
    "def plot_with_carnatic_bars_with_peaks(segment, carnatic_frequencies, color='lime'):\n",
    "    plt.style.use('dark_background')  # Dark mode\n",
    "\n",
    "    bars = list(carnatic_frequencies.values())\n",
    "\n",
    "    # Find relevant frequency range\n",
    "    min_freq = get_closest_frequency(np.nanmin(segment), carnatic_frequencies)\n",
    "    max_freq = get_closest_frequency(np.nanmax(segment), carnatic_frequencies)\n",
    "    newbars = [i for i in bars if min_freq <= i <= max_freq]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x_vals = np.arange(len(segment))\n",
    "    plt.scatter(x_vals, segment, s=8, color=color)\n",
    "\n",
    "    # Peaks and valleys\n",
    "    peaks, _ = scipy_find_peaks(segment)\n",
    "    valleys, _ = scipy_find_peaks(-np.array(segment))\n",
    "    plt.plot(peaks, segment[peaks], \"o\", markersize=4, color=\"cyan\", label=\"Peaks/Valleys\")\n",
    "    plt.plot(valleys, segment[valleys], \"o\", markersize=4, color=\"cyan\")\n",
    "\n",
    "    # Plot Carnatic bars with labels\n",
    "    for freq in newbars:\n",
    "        note = get_closest_note(freq, carnatic_frequencies)\n",
    "        plt.axhline(y=freq, color='orange', linestyle='--', linewidth=0.8)\n",
    "        plt.text(0, freq, note, color='orange', fontsize=9, verticalalignment='bottom')\n",
    "\n",
    "    plt.xlabel(\"Frame Index\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.title(\"Segment with Peaks, Valleys, and Carnatic Frequency Bars\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def interpolate_with_nans(data, target_length=128):\n",
    "\n",
    "    data = np.array(data, dtype=np.float64)\n",
    "    original_length = len(data)\n",
    "    x_original = np.linspace(0, 1, original_length)\n",
    "    x_target = np.linspace(0, 1, target_length)\n",
    "    valid = ~np.isnan(data)\n",
    "    if np.count_nonzero(valid) < 2:\n",
    "        return np.full(target_length, np.nan)\n",
    "    interpolator = interp1d(x_original[valid], data[valid], kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "    interpolated = interpolator(x_target)\n",
    "    nan_mask_original = np.isnan(data)\n",
    "    nan_mask_interpolated = np.interp(x_target, x_original, nan_mask_original.astype(float)) > 0.5\n",
    "    interpolated[nan_mask_interpolated] = np.nan\n",
    "    return interpolated\n",
    "\n",
    "def play_segment(beat_audio, beat_sr, start_frame,end_frame):\n",
    "    # Ensure the beat_index is valid\n",
    "\n",
    "\n",
    "    # Convert frames to time\n",
    "    start_time = librosa.frames_to_time(start_frame, sr=beat_sr)\n",
    "    end_time = librosa.frames_to_time(end_frame, sr=beat_sr)\n",
    "\n",
    "    # Convert time to sample indices\n",
    "    start_sample = int(start_time * beat_sr)\n",
    "    end_sample = int(end_time * beat_sr)\n",
    "\n",
    "    # Slice the audio segment\n",
    "    audio_segment = beat_audio[start_sample:end_sample]\n",
    "\n",
    "    # Play the audio segment\n",
    "    ipd.display(ipd.Audio(audio_segment, rate=beat_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This code adds a new column to the Master_Crepe.csv file \n",
    "that normalizes the frequency values based on the tonic of each song.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Master_Crepe_Mayamalavagowlai.csv\")\n",
    "normalized_values = []\n",
    "for tonic, freq in zip(df[\"Tonic\"], df[\"Frequency\"]):\n",
    "    base = get_carnatic_frequencies(tonic)[\"Sa\"]\n",
    "    if base and base > 0:\n",
    "        normalized_values.append(freq / base)\n",
    "    else:\n",
    "        normalized_values.append(None)  # or np.nan\n",
    "\n",
    "df[\"Tonic_Normalized_Frequency\"] = normalized_values\n",
    "df.to_csv(\"Master_Crepe_Mayamalavagowlai.csv\", index=False)\n",
    "print(\"✅ Tonic_Normalized_Frequency column added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from fastdtw import fastdtw\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import Counter, defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json, os\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "MASTER = \"Master_Crepe_Mayamalavagowlai.csv\"\n",
    "CARVA  = \"carva_Mayamalavagowlai.csv\"\n",
    "N_JOBS = os.cpu_count()        # change to e.g. 4 to pin the pool size\n",
    "df_master = pd.read_csv(MASTER)\n",
    "\n",
    "def non_overlapping_segments(conf, window_size, hop_size):\n",
    "    segments = []\n",
    "    indices = []\n",
    "    i = 0\n",
    "    while i < len(conf) - window_size:\n",
    "        segment = conf[i:i + window_size]\n",
    "        if np.isnan(segment).any():\n",
    "            i += hop_size\n",
    "            continue\n",
    "        segments.append(segment)\n",
    "        indices.append(i)\n",
    "        i += window_size  # skip all overlapping windows\n",
    "    return np.array(segments), np.array(indices)\n",
    "\n",
    "def dtw_distance_matrix(segments):\n",
    "    n = len(segments)\n",
    "    dists = np.zeros((n, n))\n",
    "    for i in tqdm(range(n)):\n",
    "        for j in range(i+1, n):\n",
    "            dist, _ = fastdtw(segments[i], segments[j])\n",
    "            dists[i, j] = dist\n",
    "            dists[j, i] = dist\n",
    "    return dists\n",
    "\n",
    "def extract_notes_from_conf(\n",
    "        conf, \n",
    "        initial_window_size, \n",
    "        decay_size, \n",
    "        min_window_size, \n",
    "        outlier_threshold,\n",
    "        similairity_threshold=100):\n",
    "    \"\"\"\n",
    "    Normal clustering of segments with decreasing window size.\n",
    "    \"\"\"\n",
    "    conf = conf.copy()\n",
    "    remaining_conf = conf.copy()\n",
    "    all_removed_segments = []\n",
    "\n",
    "    window_size = initial_window_size\n",
    "    global_label_offset = 0\n",
    "\n",
    "    total_iters = (initial_window_size - min_window_size) // decay_size + 1\n",
    "    iter_count = 0\n",
    "\n",
    "    while window_size >= min_window_size:\n",
    "        iter_count += 1\n",
    "        print(f\"Iteration {iter_count}/{total_iters} — Window Size: {window_size}\")\n",
    "\n",
    "        hop_size = int(window_size / 12)\n",
    "        segments, segment_starts = non_overlapping_segments(remaining_conf, window_size, hop_size)\n",
    "        \n",
    "        if len(segments) == 0:\n",
    "            print(\"  Skipped — no valid segments\")\n",
    "            window_size -= decay_size\n",
    "            continue\n",
    "\n",
    "        dtw_dists = dtw_distance_matrix(segments)\n",
    "\n",
    "        if len(segments) < 2:\n",
    "            return remaining_conf, all_removed_segments  # <-- add this safeguard\n",
    "\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=similairity_threshold,\n",
    "            metric='precomputed',\n",
    "            linkage='average'\n",
    "        )\n",
    "        labels = clustering.fit_predict(dtw_dists)\n",
    "        labels = clustering.fit_predict(dtw_dists)\n",
    "\n",
    "        cluster_dict = defaultdict(list)\n",
    "        cluster_origins = defaultdict(list)\n",
    "\n",
    "        for seg, start_idx, lbl in zip(segments, segment_starts, labels):\n",
    "            cluster_dict[lbl].append(seg)\n",
    "            cluster_origins[lbl].append(start_idx)\n",
    "\n",
    "        clustered = False\n",
    "        for label, starts in cluster_origins.items():\n",
    "            if len(starts) >= outlier_threshold:\n",
    "                clustered = True\n",
    "                global_label = global_label_offset + label\n",
    "                for i in starts:\n",
    "                    remaining_conf[i:i + window_size] = np.full(window_size, np.nan)\n",
    "                    all_removed_segments.append([i, i + window_size, global_label])\n",
    "\n",
    "        if clustered:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, removed some segments.\")\n",
    "        else:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, but none met the threshold.\")\n",
    "\n",
    "        global_label_offset += len(set(labels))\n",
    "        window_size -= decay_size\n",
    "\n",
    "    return remaining_conf, all_removed_segments\n",
    "\n",
    "def extract_notes_from_conf_pca(conf,\n",
    "    initial_window_size = 128,\n",
    "    decay_size = 8,\n",
    "    min_window_size = 32,\n",
    "    hop_factor = 12,\n",
    "    outlier_threshold = 3,\n",
    "    similarity_threshold = 0.7,\n",
    "    pca_components = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    PCA-based clustering of segments with decreasing window size.\n",
    "    \"\"\"\n",
    "    conf = conf.copy()\n",
    "    remaining_conf = conf.copy()\n",
    "    all_removed_segments = []\n",
    "    global_label_offset = 0\n",
    "\n",
    "    window_size = initial_window_size\n",
    "    total_iters = (initial_window_size - min_window_size) // decay_size + 1\n",
    "    iter_count = 0\n",
    "\n",
    "    while window_size >= min_window_size:\n",
    "        iter_count += 1\n",
    "        print(f\"Iteration {iter_count}/{total_iters} — Window Size: {window_size}\")\n",
    "\n",
    "        hop_size = max(1, int(window_size / hop_factor))\n",
    "        segments, segment_starts = non_overlapping_segments(remaining_conf, window_size, hop_size)\n",
    "\n",
    "        if len(segments) < 2:\n",
    "            print(\"  Skipped — not enough segments.\")\n",
    "            window_size -= decay_size\n",
    "            continue\n",
    "\n",
    "        # === PCA transformation ===\n",
    "        X_abs = np.stack(segments)\n",
    "        X_shape = X_abs - np.mean(X_abs, axis=1, keepdims=True)\n",
    "        X_combined = np.concatenate([X_abs, X_shape], axis=1)\n",
    "\n",
    "        pca = PCA(n_components=min(pca_components, X_combined.shape[1]))\n",
    "        X_pca = pca.fit_transform(X_combined)\n",
    "\n",
    "        dist_matrix = squareform(pdist(X_pca, metric='euclidean'))\n",
    "\n",
    "        # === Clustering ===\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=similarity_threshold,\n",
    "            metric='precomputed',\n",
    "            linkage='average'\n",
    "        )\n",
    "        labels = clustering.fit_predict(dist_matrix)\n",
    "\n",
    "        cluster_origins = defaultdict(list)\n",
    "        for start_idx, lbl in zip(segment_starts, labels):\n",
    "            cluster_origins[lbl].append(start_idx)\n",
    "\n",
    "        clustered = False\n",
    "        for label, starts in cluster_origins.items():\n",
    "            if len(starts) >= outlier_threshold:\n",
    "                clustered = True\n",
    "                global_label = global_label_offset + label\n",
    "                for i in starts:\n",
    "                    remaining_conf[i:i + window_size] = np.full(window_size, np.nan)\n",
    "                    all_removed_segments.append([i, i + window_size, global_label])\n",
    "\n",
    "        if clustered:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, removed segments.\")\n",
    "        else:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, but none met the threshold.\")\n",
    "\n",
    "        global_label_offset += len(set(labels))\n",
    "        window_size -= decay_size\n",
    "\n",
    "    return remaining_conf, all_removed_segments\n",
    "\n",
    "def plot_colored_segments(original_conf, removed_segments, residual_conf=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # Assign a color to each cluster\n",
    "    cluster_labels = sorted(set(lbl for _, _, lbl in removed_segments))\n",
    "    cmap = cm.get_cmap('tab20', len(cluster_labels))\n",
    "    cluster_to_color = {label: cmap(i) for i, label in enumerate(cluster_labels)}\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(original_conf, label=\"Original\", alpha=0.2, color='gray')\n",
    "\n",
    "    # Plot segments grouped by cluster label\n",
    "    for start, end, label in removed_segments:\n",
    "        plt.plot(range(start, end), original_conf[start:end], color=cluster_to_color[label], label=f\"Cluster {label}\")\n",
    "\n",
    "    # Plot residual if given\n",
    "    if residual_conf is not None:\n",
    "        plt.plot(residual_conf, label=\"Residual\", linewidth=2, color='black')\n",
    "\n",
    "    # Create a legend without duplicate labels\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.title(\"Clusters of Repeating Notes\")\n",
    "    plt.xlabel(\"Time Index\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def remap_cluster_labels(removed_segments):\n",
    "    old_labels = sorted(set(lbl for _, _, lbl in removed_segments))\n",
    "    label_map  = {old: new for new, old in enumerate(old_labels)}\n",
    "    return [(s, e, label_map[lbl]) for s, e, lbl in removed_segments]\n",
    "\n",
    "def process_one_song(song_idx):\n",
    "    \"\"\"Worker: returns a list[dict] for one song.\"\"\"\n",
    "    song_df   = df_master[df_master[\"Index\"] == song_idx].reset_index(drop=True)\n",
    "    audio_path = song_df.loc[0, \"AudioPath\"]\n",
    "    tonic_norm = song_df[\"Tonic_Normalized_Frequency\"].values\n",
    "\n",
    "    residual_conf, removed_segments_local = extract_notes_from_conf(\n",
    "        tonic_norm,\n",
    "        initial_window_size = 60,\n",
    "        decay_size          = 2,\n",
    "        min_window_size     = 15,\n",
    "        outlier_threshold   = 2,\n",
    "        similairity_threshold = 0.5\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for start, end, lbl in remap_cluster_labels(removed_segments_local):\n",
    "        rows.append({\n",
    "            \"Index\"      : int(song_idx),\n",
    "            \"AudioPath\"  : audio_path,\n",
    "            \"SegmentList\": json.dumps(tonic_norm[start:end].tolist()),\n",
    "            \"StartFrame\" : int(start),\n",
    "            \"EndFrame\"   : int(end - 1),\n",
    "            \"Label\"      : lbl\n",
    "        })\n",
    "    return rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fdc595",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram_with_crepe()\n",
    "\n",
    "# \"\"\"This code here is just to see if the DTW is working or not, you can ignore this part, it is just for testing purposes\"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # 1️⃣ Load Master file\n",
    "# df_master = pd.read_csv(\"Master_Crepe_Mayamalavagowlai.csv\")\n",
    "\n",
    "# # 2️⃣ Get the first song's index (lowest Index)\n",
    "# song_idx = df_master[\"Index\"].min()+3\n",
    "\n",
    "# # 3️⃣ Filter rows for this song & take first 1000 rows\n",
    "# first_song_df = df_master[df_master[\"Index\"] == song_idx].head(4000).reset_index(drop=True)\n",
    "\n",
    "# # 4️⃣ Extract the frequency contour\n",
    "# tonic_norm = first_song_df[\"Tonic_Normalized_Frequency\"].values\n",
    "\n",
    "# # 5️⃣ Run your extraction\n",
    "# residual_conf, removed_segments_local = extract_notes_from_conf_pca(\n",
    "#     tonic_norm,\n",
    "#     initial_window_size = 60,\n",
    "#     decay_size          = 2,\n",
    "#     min_window_size     = 15,\n",
    "#     hop_factor= 12,\n",
    "#     outlier_threshold   = 2,\n",
    "#     similarity_threshold = 0.4,\n",
    "#     pca_components= 4\n",
    "# )\n",
    "\n",
    "\n",
    "# # 6️⃣ Remap labels for neat coloring\n",
    "# removed_segments_local = remap_cluster_labels(removed_segments_local)\n",
    "\n",
    "# # 7️⃣ Plot\n",
    "# plot_colored_segments(\n",
    "#     original_conf = tonic_norm,\n",
    "#     removed_segments = removed_segments_local,#REmoved segments local has a list of tuples (start, end, label)\n",
    "#     residual_conf = residual_conf\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" This code is to play the segments of the song that were removed by the DTW algorithm.\n",
    "# This is useful to listen to the segments that were clustered together and removed from the original frequency contour.\"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import librosa\n",
    "# import IPython.display as ipd\n",
    "\n",
    "# def play_segment(csvfile, audio_index, sr, start_idx, end_idx):\n",
    "#     df = pd.read_csv(csvfile)\n",
    "#     audio_path = df.loc[df['Index'] == audio_index, 'AudioPath'].values[0]\n",
    "\n",
    "#     # For this song, get the original spec_time (after confidence filter)\n",
    "#     spec_time = df[df['Index'] == audio_index][\"Time\"].values\n",
    "\n",
    "#     # Map indices to real time\n",
    "#     start_time = spec_time[start_idx]\n",
    "#     end_time   = spec_time[end_idx]\n",
    "\n",
    "#     audio = librosa.load(audio_path, sr=sr)[0]\n",
    "\n",
    "#     start_sample = int(start_time * sr)\n",
    "#     end_sample   = int(end_time * sr)\n",
    "\n",
    "#     return ipd.Audio(audio[start_sample:end_sample], rate=sr)\n",
    "\n",
    "\n",
    "\n",
    "# # Suppose removed_segments_local is given\n",
    "# # Example: [(start, end, cluster)]\n",
    "\n",
    "# # Compute unique clusters safely\n",
    "# clusters = sorted(set(seg[2] for seg in removed_segments_local))\n",
    "# for cluster_id in clusters:\n",
    "#     print(f\"=== Playing Cluster {cluster_id} ===\")\n",
    "#     for segment in removed_segments_local:\n",
    "#         if segment[2] == cluster_id:\n",
    "#             display(\n",
    "#                 play_segment(\n",
    "#                     'Master_Crepe_Mayamalavagowlai.csv',\n",
    "#                     audio_index=song_idx,\n",
    "#                     sr=44100,\n",
    "#                     start_idx=segment[0],\n",
    "#                     end_idx=segment[1]\n",
    "#                 )\n",
    "#             )\n",
    "#     print(\"-----------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa319c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code here is the MAIN PART of the script that processes all songs in parallel and saves the results to a CSV file.\"\"\"\n",
    "\n",
    "\n",
    "def extract_notes_from_conf_pca(conf,\n",
    "    initial_window_size = 128,\n",
    "    decay_size = 8,\n",
    "    min_window_size = 32,\n",
    "    hop_factor = 12,\n",
    "    outlier_threshold = 3,\n",
    "    similarity_threshold = 0.04,\n",
    "    pca_components = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    PCA-based clustering of segments with decreasing window size.\n",
    "    \"\"\"\n",
    "    conf = conf.copy()\n",
    "    remaining_conf = conf.copy()\n",
    "    all_removed_segments = []\n",
    "    global_label_offset = 0\n",
    "\n",
    "    window_size = initial_window_size\n",
    "    total_iters = (initial_window_size - min_window_size) // decay_size + 1\n",
    "    iter_count = 0\n",
    "\n",
    "    while window_size >= min_window_size:\n",
    "        iter_count += 1\n",
    "        print(f\"Iteration {iter_count}/{total_iters} — Window Size: {window_size}\")\n",
    "\n",
    "        hop_size = max(1, int(window_size / hop_factor))\n",
    "        segments, segment_starts = non_overlapping_segments(remaining_conf, window_size, hop_size)\n",
    "\n",
    "        if len(segments) < 2:\n",
    "            print(\"  Skipped — not enough segments.\")\n",
    "            window_size -= decay_size\n",
    "            continue\n",
    "\n",
    "        # === PCA transformation ===\n",
    "        X_abs = np.stack(segments)\n",
    "        X_shape = X_abs - np.mean(X_abs, axis=1, keepdims=True)\n",
    "        X_combined = np.concatenate([X_abs, X_shape], axis=1)\n",
    "\n",
    "        pca = PCA(n_components=min(pca_components, X_combined.shape[1]))\n",
    "        X_pca = pca.fit_transform(X_combined)\n",
    "\n",
    "        dist_matrix = squareform(pdist(X_pca, metric='euclidean'))\n",
    "\n",
    "        # === Clustering ===\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=similarity_threshold,\n",
    "            metric='precomputed',\n",
    "            linkage='average'\n",
    "        )\n",
    "        labels = clustering.fit_predict(dist_matrix)\n",
    "\n",
    "        cluster_origins = defaultdict(list)\n",
    "        for start_idx, lbl in zip(segment_starts, labels):\n",
    "            cluster_origins[lbl].append(start_idx)\n",
    "\n",
    "        clustered = False\n",
    "        for label, starts in cluster_origins.items():\n",
    "            if len(starts) >= outlier_threshold:\n",
    "                clustered = True\n",
    "                global_label = global_label_offset + label\n",
    "                for i in starts:\n",
    "                    remaining_conf[i:i + window_size] = np.full(window_size, np.nan)\n",
    "                    all_removed_segments.append([i, i + window_size, global_label])\n",
    "\n",
    "        if clustered:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, removed segments.\")\n",
    "        else:\n",
    "            print(f\"  Clusters found: {len(set(labels))}, but none met the threshold.\")\n",
    "\n",
    "        global_label_offset += len(set(labels))\n",
    "        window_size -= decay_size\n",
    "\n",
    "    return remaining_conf, all_removed_segments\n",
    "\n",
    "def process_one_song_pca(song_idx):\n",
    "    \"\"\"Worker: returns a list[dict] for one song.\"\"\"\n",
    "    song_df   = df_master[df_master[\"Index\"] == song_idx].reset_index(drop=True)\n",
    "    audio_path = song_df.loc[0, \"AudioPath\"]\n",
    "    tonic_norm = song_df[\"Tonic_Normalized_Frequency\"].values\n",
    "\n",
    "    residual_conf, removed_segments_local = extract_notes_from_conf_pca(\n",
    "    tonic_norm,\n",
    "    initial_window_size = 60,\n",
    "    decay_size          = 2,\n",
    "    min_window_size     = 15,\n",
    "    hop_factor= 12,\n",
    "    outlier_threshold   = 2,\n",
    "    similarity_threshold = 0.04,\n",
    "    pca_components= 15\n",
    ")\n",
    "\n",
    "    rows = []\n",
    "    for start, end, lbl in remap_cluster_labels(removed_segments_local):\n",
    "        rows.append({\n",
    "            \"Index\"      : int(song_idx),\n",
    "            \"AudioPath\"  : audio_path,\n",
    "            \"SegmentList\": json.dumps(tonic_norm[start:end].tolist()),\n",
    "            \"StartFrame\" : int(start),\n",
    "            \"EndFrame\"   : int(end - 1),\n",
    "            \"Label\"      : lbl\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------- parallel map -------------\n",
    "song_indices = sorted(df_master[\"Index\"].unique())\n",
    "\n",
    "results = Parallel(n_jobs=N_JOBS, backend=\"loky\")(\n",
    "            delayed(process_one_song_pca)(idx) for idx in tqdm(song_indices)\n",
    "          )\n",
    "\n",
    "# flatten list‑of‑lists -> list‑of‑dicts\n",
    "flat_rows = [row for song_rows in results for row in song_rows]\n",
    "carva_df  = pd.DataFrame(flat_rows)\n",
    "\n",
    "# ------------- write / append -------------\n",
    "if Path(CARVA).exists():\n",
    "    carva_df.to_csv(CARVA, mode=\"a\", index=False, header=False)\n",
    "else:\n",
    "    carva_df.to_csv(CARVA, index=False)\n",
    "\n",
    "print(f\"✅ Parallel run finished — {len(carva_df)} segments across {len(song_indices)} songs saved to {CARVA}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbd49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "# --------- Utility Functions ---------\n",
    "\n",
    "def interpolate_list(lst, target_len):\n",
    "    original_len = len(lst)\n",
    "    if original_len == 0:\n",
    "        return [0] * target_len\n",
    "    return list(np.interp(np.linspace(0, original_len - 1, target_len),\n",
    "                          np.arange(original_len), lst))\n",
    "\n",
    "def non_overlapping_segments(arr, window_size, hop_size):\n",
    "    segments = []\n",
    "    starts = []\n",
    "    for start in range(0, len(arr) - window_size + 1, hop_size):\n",
    "        segment = arr[start:start + window_size]\n",
    "        if not np.any(np.isnan(segment)):\n",
    "            segments.append(segment)\n",
    "            starts.append(start)\n",
    "    return segments, starts\n",
    "\n",
    "# --------- Main Clustering Function ---------\n",
    "\n",
    "def extract_notes_from_conf_once_pca(conf, window_size=128, hop_size=128,\n",
    "                                     outlier_threshold=3, similarity_threshold=0.7,\n",
    "                                     pca_components=10):\n",
    "    conf = conf.copy()\n",
    "    remaining_conf = conf.copy()\n",
    "    all_removed_segments = []\n",
    "\n",
    "    segments, segment_starts = non_overlapping_segments(remaining_conf, window_size, hop_size)\n",
    "\n",
    "    if len(segments) < 2:\n",
    "        print(\"Not enough segments to compare.\")\n",
    "        return remaining_conf, all_removed_segments, []\n",
    "\n",
    "    X_abs = np.stack(segments)\n",
    "    X_shape = X_abs - np.mean(X_abs, axis=1, keepdims=True)\n",
    "    X_combined = np.concatenate([X_abs, X_shape], axis=1)\n",
    "\n",
    "    pca = PCA(n_components=min(pca_components, X_combined.shape[1]))\n",
    "    X_pca = pca.fit_transform(X_combined)\n",
    "\n",
    "    dist_matrix = squareform(pdist(X_pca, metric='euclidean'))\n",
    "\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=similarity_threshold,\n",
    "        metric='precomputed',\n",
    "        linkage='average'\n",
    "    )\n",
    "    labels = clustering.fit_predict(dist_matrix)\n",
    "\n",
    "    cluster_origins = defaultdict(list)\n",
    "    for start_idx, lbl in zip(segment_starts, labels):\n",
    "        cluster_origins[lbl].append(start_idx)\n",
    "\n",
    "    for label, starts in cluster_origins.items():\n",
    "        if len(starts) >= outlier_threshold:\n",
    "            for i in starts:\n",
    "                remaining_conf[i:i + window_size] = np.full(window_size, np.nan)\n",
    "                all_removed_segments.append([i, i + window_size, label])\n",
    "\n",
    "    print(f\"Clusters found: {len(set(labels))}. Removed {len(all_removed_segments)} segments.\")\n",
    "    return remaining_conf, all_removed_segments, labels\n",
    "\n",
    "# --------- CSV Update Function ---------\n",
    "\n",
    "def update_carva_csv_with_labels(file_path, labels):\n",
    "    carva_df = pd.read_csv(file_path)\n",
    "\n",
    "    if len(labels) != len(carva_df):\n",
    "        raise ValueError(f\"Label count ({len(labels)}) does not match row count ({len(carva_df)})\")\n",
    "\n",
    "    carva_df['Second Labels'] = labels\n",
    "    carva_df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated {file_path} with Second Labels.\")\n",
    "\n",
    "# --------- Main Driver Code ---------\n",
    "\n",
    "file_path = 'carva_Mayamalavagowlai.csv'\n",
    "second_phase_segments = []\n",
    "second_queue = []\n",
    "\n",
    "carva_df = pd.read_csv(file_path)\n",
    "segments = carva_df['SegmentList']\n",
    "\n",
    "for segment in segments:\n",
    "    if pd.isna(segment):\n",
    "        continue  # skip blank entries\n",
    "    try:\n",
    "        seg = ast.literal_eval(segment)\n",
    "    except:\n",
    "        continue  # skip invalid entries\n",
    "    interp = interpolate_list(seg, 50)\n",
    "    second_phase_segments.append(interp)\n",
    "    second_queue += interp  # extend the full signal queue\n",
    "\n",
    "# --------- Debug Info ---------\n",
    "print(\"Total rows in CSV:\", len(carva_df))\n",
    "print(\"Valid segments used:\", len(second_phase_segments))\n",
    "print(\"Total length of signal:\", len(second_queue))\n",
    "print(\"Window size: 50, hop size: 50\")\n",
    "print(\"Expected segments:\", len(second_queue) // 50)\n",
    "\n",
    "# --------- Run Clustering ---------\n",
    "_, _, labels = extract_notes_from_conf_once_pca(second_queue, 50, 50)\n",
    "\n",
    "print(\"Number of labels:\", len(labels))\n",
    "\n",
    "# --------- Update CSV ---------\n",
    "update_carva_csv_with_labels(file_path, labels)\n",
    "file_path = 'carva_Mayamalavagowlai.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Compute interpolated segments\n",
    "interpolated = []\n",
    "for val in df[\"SegmentList\"]:\n",
    "    try:\n",
    "        segment = ast.literal_eval(val)\n",
    "        interpolated.append(interpolate_list(segment, 50))\n",
    "    except:\n",
    "        interpolated.append([])\n",
    "\n",
    "# Insert new column after \"SegmentList\"\n",
    "segment_index = df.columns.get_loc(\"SegmentList\")\n",
    "df.insert(segment_index + 1, \"Interpolated_SegmentList\", interpolated)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(file_path, index=False)\n",
    "print(\"✅ Interpolated_SegmentList column added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "def play_second_phase_clusters(\n",
    "    carva_csv: str,\n",
    "    master_csv: str,\n",
    "    sr: int = 44100\n",
    "):\n",
    "    \"\"\"\n",
    "    Play back motifs grouped by second-phase clustering from your carva CSV.\n",
    "    Uses StartFrame/EndFrame and Index to map back to real audio.\n",
    "\n",
    "    Args:\n",
    "        carva_csv: Path to carva CSV with 'Index', 'AudioPath', 'StartFrame', 'EndFrame', 'Second Labels'.\n",
    "        master_csv: Path to Master CSV with Time column.\n",
    "        sr: Desired sampling rate for playback.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    carva_df = pd.read_csv(carva_csv)\n",
    "    master_df = pd.read_csv(master_csv)\n",
    "\n",
    "    # Find unique clusters\n",
    "    clusters = sorted(carva_df['Second Labels'].dropna().unique())\n",
    "\n",
    "    for cluster_id in clusters:\n",
    "        print(f\"\\n=== Playing Cluster {cluster_id} ===\")\n",
    "        cluster_rows = carva_df[carva_df['Second Labels'] == cluster_id]\n",
    "\n",
    "        for _, row in cluster_rows.iterrows():\n",
    "            audio_path = row['AudioPath']\n",
    "            index = row['Index']\n",
    "            start_idx = int(row['StartFrame'])\n",
    "            end_idx = int(row['EndFrame'])\n",
    "\n",
    "            # Get real time from Master CSV for this song\n",
    "            spec_time = master_df[master_df['Index'] == index]['Time'].values\n",
    "\n",
    "            # Protect against index overflow\n",
    "            start_time = spec_time[start_idx] if start_idx < len(spec_time) else 0\n",
    "            end_time = spec_time[end_idx] if end_idx < len(spec_time) else spec_time[-1]\n",
    "\n",
    "            # Load audio\n",
    "            audio, _ = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "            start_sample = int(start_time * sr)\n",
    "            end_sample = int(end_time * sr)\n",
    "\n",
    "            display(ipd.Audio(audio[start_sample:end_sample], rate=sr))\n",
    "\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import ast\n",
    "import re\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def clean_np_float_list(seg_str):\n",
    "    \"\"\"\n",
    "    Convert a stringified list with np.float64(...) entries into a proper list of floats.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'np\\.float64\\(([^)]+)\\)', r'\\1', seg_str)\n",
    "    return np.array(ast.literal_eval(cleaned), dtype=float)\n",
    "\n",
    "def play_specific_second_phase_cluster(\n",
    "    carva_csv: str,\n",
    "    master_csv: str,\n",
    "    cluster_number: int,\n",
    "    carnatic_frequencies: dict,\n",
    "    sr: int = 44100\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot interpolated frequency segments (overlapping from x=0) of a specific cluster, \n",
    "    showing extrema (peaks & valleys), then play the audio segments.\n",
    "    \"\"\"\n",
    "    carva_df = pd.read_csv(carva_csv)\n",
    "    master_df = pd.read_csv(master_csv)\n",
    "\n",
    "    cluster_rows = carva_df[carva_df['Second Labels'] == cluster_number]\n",
    "\n",
    "    if cluster_rows.empty:\n",
    "        print(f\"⚠️ No entries found for cluster {cluster_number}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n🎼 Plotting interpolated segments (overlapping) for Cluster {cluster_number}...\")\n",
    "\n",
    "    # Plot setup\n",
    "    plt.style.use('dark_background')\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    bars = list(carnatic_frequencies.values())\n",
    "    all_freqs = []\n",
    "\n",
    "    for _, row in cluster_rows.iterrows():\n",
    "        seg_str = row.get('Interpolated_SegmentList', None)\n",
    "        if pd.isna(seg_str):\n",
    "            continue\n",
    "        try:\n",
    "            segment = clean_np_float_list(seg_str)\n",
    "            x = np.arange(len(segment))\n",
    "            plt.plot(x, segment, alpha=0.6, linewidth=1)\n",
    "\n",
    "            # Plot extrema\n",
    "            peaks, _ = find_peaks(segment)\n",
    "            valleys, _ = find_peaks(-segment)\n",
    "            plt.plot(x[peaks], segment[peaks], \"o\", color=\"cyan\", markersize=4)\n",
    "            plt.plot(x[valleys], segment[valleys], \"o\", color=\"cyan\", markersize=4)\n",
    "\n",
    "            all_freqs.extend(segment)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping segment due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Carnatic swara bars\n",
    "    if all_freqs:\n",
    "        min_freq = np.min(all_freqs)\n",
    "        max_freq = np.max(all_freqs)\n",
    "        newbars = [i for i in bars if min_freq <= i <= max_freq]\n",
    "\n",
    "        for freq in newbars:\n",
    "            note = get_closest_note(freq, carnatic_frequencies)\n",
    "            plt.axhline(y=freq, color='orange', linestyle='--', linewidth=0.8)\n",
    "            plt.text(0, freq, note, color='orange', fontsize=9, verticalalignment='bottom')\n",
    "\n",
    "    plt.title(f\"Overlapping Interpolated Frequency Plots — Cluster {cluster_number}\")\n",
    "    plt.xlabel(\"Frame Index\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Playback\n",
    "    print(f\"\\n🎧 Playing Cluster {cluster_number} ===\")\n",
    "    for _, row in cluster_rows.iterrows():\n",
    "        audio_path = row['AudioPath']\n",
    "        index = row['Index']\n",
    "        start_idx = int(row['StartFrame'])\n",
    "        end_idx = int(row['EndFrame'])\n",
    "        swara = row['Swara'] if 'Swara' in row and pd.notna(row['Swara']) else \"Not labeled\"\n",
    "\n",
    "        print(f\"\\n📄 AudioPath: {audio_path}\")\n",
    "        print(f\"🎵 Swara: {swara}\")\n",
    "\n",
    "        spec_time = master_df[master_df['Index'] == index]['Time'].values\n",
    "        start_time = spec_time[start_idx] if start_idx < len(spec_time) else 0\n",
    "        end_time   = spec_time[end_idx] if end_idx < len(spec_time) else spec_time[-1]\n",
    "\n",
    "        try:\n",
    "            audio, _ = librosa.load(audio_path, sr=sr)\n",
    "            start_sample = int(start_time * sr)\n",
    "            end_sample = int(end_time * sr)\n",
    "            display(ipd.Audio(audio[start_sample:end_sample], rate=sr))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load audio: {e}\")\n",
    "\n",
    "    print(\"\\n-----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "carnatic_ratios = {\n",
    "    'sa': 0.5*1.0,    # Tonic (Sa)\n",
    "    'ri1': 0.5*16/15, # Ri1\n",
    "    'ri2': 0.5*9/8,  # Ri2\n",
    "    'ga1': 0.5*6/5,  # Ga1\n",
    "    'ga2': 0.5*5/4, # Ga2\n",
    "    'ma1': 0.5*4/3, # Ma1\n",
    "    'ma2': 0.5*45/32,   # Ma2\n",
    "    'pa': 0.5*3/2,    # Pa\n",
    "    'da1': 0.5*8/5, # Dha1\n",
    "    'da2': 0.5*5/3, # Dha2\n",
    "    'ni1': 0.5*16/9, # Ni1\n",
    "    'ni2': 0.5*15/8,   # Ni2\n",
    "\n",
    "    'Sa': 1.0,    # Tonic (Sa)\n",
    "    'Ri1': 16/15, # Ri1\n",
    "    'Ri2': 9/8,  # Ri2\n",
    "    'Ga1': 6/5,  # Ga1\n",
    "    'Ga2': 5/4, # Ga2\n",
    "    'Ma1': 4/3, # Ma1\n",
    "    'Ma2': 45/32,   # Ma2\n",
    "    'Pa': 3/2,    # Pa\n",
    "    'Da1': 8/5, # Dha1\n",
    "    'Da2': 5/3, # Dha2\n",
    "    'Ni1': 16/9, # Ni1\n",
    "    'Ni2': 15/8,   # Ni2\n",
    "\n",
    "    'SA': 2.0,   # Octave higher (Sa)\n",
    "    'RI1': 2*16/15, # Ri1\n",
    "    'RI2': 2*9/8,  # Ri2\n",
    "    'GA1': 2*6/5,  # Ga1\n",
    "    'GA2': 2*5/4, # Ga2\n",
    "    'MA1': 2*4/3, # Ma1\n",
    "    'MA2': 2*45/32,   # Ma2\n",
    "    'PA': 2*3/2,    # Pa\n",
    "    'DA1': 2*8/5, # Dha1\n",
    "    'DA2': 2*5/3, # Dha2\n",
    "    'NI1': 2*16/9, # Ni1\n",
    "    'NI2': 2*15/8,   \n",
    "}\n",
    "\n",
    "play_specific_second_phase_cluster(\n",
    "    carnatic_frequencies=carnatic_ratios,\n",
    "    carva_csv=\"carva_Mayamalavagowlai.csv\",\n",
    "    master_csv=\"Master_Crepe_Mayamalavagowlai.csv\",\n",
    "    cluster_number=1,\n",
    "    sr=44100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_swara_for_cluster(\n",
    "    carva_csv: str,\n",
    "    cluster_number: int,\n",
    "    swara_label: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Adds or updates a column 'Swara' in the carva CSV file.\n",
    "    For all rows where 'Second Labels' == cluster_number, the value is set to swara_label.\n",
    "\n",
    "    Args:\n",
    "        carva_csv: Path to the carva CSV.\n",
    "        cluster_number: The cluster number to label.\n",
    "        swara_label: The string label to assign (e.g., 'S', 'R2', 'G3').\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(carva_csv)\n",
    "\n",
    "    # Create the 'Swara' column if it doesn't exist\n",
    "    if 'Swara' not in df.columns:\n",
    "        df['Swara'] = \"\"\n",
    "\n",
    "    # Apply the swara label to matching cluster rows\n",
    "    match_count = (df['Second Labels'] == cluster_number).sum()\n",
    "    if match_count == 0:\n",
    "        print(f\"⚠️ No rows found for cluster {cluster_number}. Nothing updated.\")\n",
    "    else:\n",
    "        df.loc[df['Second Labels'] == cluster_number, 'Swara'] = swara_label\n",
    "        df.to_csv(carva_csv, index=False)\n",
    "        print(f\"✅ Labeled {match_count} rows in cluster {cluster_number} as '{swara_label}'.\")\n",
    "\n",
    "\n",
    "label_swara_for_cluster(\n",
    "    carva_csv=\"carva_Mayamalavagowlai.csv\",\n",
    "    cluster_number=14,   # The cluster you want to label\n",
    "    swara_label=\"Ni2 Da1 Pa\"    # The swara label to apply\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d18335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_empty_dh_csvs(root_directory):\n",
    "    \"\"\"\n",
    "    Traverses the directory, finds folders ending with '_links',\n",
    "    and creates an empty CSV file named DH_RobotName.csv inside each.\n",
    "\n",
    "    Args:\n",
    "        root_directory (str): The path to the 'Robot3DModels' directory.\n",
    "    \"\"\"\n",
    "    print(f\"Starting to process folders in: {root_directory}\")\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_directory):\n",
    "        # Get the base name of the current directory\n",
    "        folder_name = os.path.basename(dirpath)\n",
    "\n",
    "        # Check if the current directory name ends with '_links'\n",
    "        if folder_name.endswith('_links'):\n",
    "            # Remove '_links' to get the robot name\n",
    "            robot_name = folder_name[:-len('_links')]\n",
    "            \n",
    "            # Construct the CSV filename\n",
    "            csv_filename = f\"DH_{robot_name}.csv\"\n",
    "            \n",
    "            # Construct the full path for the new CSV file (inside the current folder)\n",
    "            csv_file_path = os.path.join(dirpath, csv_filename)\n",
    "\n",
    "            try:\n",
    "                # Create an empty CSV file\n",
    "                with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "                    # You can optionally write a header or initial row here if needed.\n",
    "                    # For an \"empty\" file, we just open and close it.\n",
    "                    pass \n",
    "                print(f\"Created empty file: {csv_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating file {csv_file_path}: {e}\")\n",
    "        else:\n",
    "            # Optionally, you can add a print statement here if you want to see\n",
    "            # which folders are being skipped.\n",
    "            # print(f\"Skipping non-_links folder: {dirpath}\")\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # IMPORTANT: Set this to the actual path of your 'Robot3DModels' directory.\n",
    "    root_models_dir = r'C:\\Desktop\\VisualStudioFiles\\RobotSimulator-2025(1)\\RobotSimulator\\RobotSimulator\\Models\\Robot3DModels'\n",
    "    \n",
    "    create_empty_dh_csvs(root_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "def extract_dh_parameters(xml_file_path):\n",
    "    \"\"\"\n",
    "    Extracts Denavit-Hartenberg (DH) parameters from a Robot XML file.\n",
    "\n",
    "    Args:\n",
    "        xml_file_path (str): The full path to the Robot.xml file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - str: The name of the robot.\n",
    "            - list of lists: DH parameters for each joint in the order:\n",
    "                             [JointOffset (d), JointAngle (theta), LinkLength (a), TwistAngle (alpha), JointAngleMin, JointAngleMax].\n",
    "                             Returns None if parsing fails or data is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        robot_name_element = root.find('Name')\n",
    "        robot_name = robot_name_element.text if robot_name_element is not None else \"UnknownRobot\"\n",
    "\n",
    "        joints_data = []\n",
    "        joints_element = root.find('Joints')\n",
    "        if joints_element is not None:\n",
    "            for joint in joints_element.findall('RevoluteJoint'):\n",
    "                joint_offset = float(joint.find('JointOffset').text) if joint.find('JointOffset') is not None else 0.0\n",
    "                joint_angle = float(joint.find('JointAngle').text) if joint.find('JointAngle') is not None else 0.0\n",
    "                link_length = float(joint.find('LinkLength').text) if joint.find('LinkLength') is not None else 0.0\n",
    "                twist_angle = float(joint.find('TwistAngle').text) if joint.find('TwistAngle') is not None else 0.0\n",
    "                joint_angle_min = float(joint.find('JointAngleMin').text) if joint.find('JointAngleMin') is not None else 0.0\n",
    "                joint_angle_max = float(joint.find('JointAngleMax').text) if joint.find('JointAngleMax') is not None else 0.0\n",
    "\n",
    "                # DH parameters order: d, theta, a, alpha, JointAngleMin, JointAngleMax\n",
    "                joints_data.append([\n",
    "                    joint_offset,\n",
    "                    joint_angle,\n",
    "                    link_length,\n",
    "                    twist_angle,\n",
    "                    joint_angle_min,\n",
    "                    joint_angle_max\n",
    "                ])\n",
    "        return robot_name, joints_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing XML file {xml_file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def update_dh_csvs_from_xml(xml_root_directory, csv_base_directory):\n",
    "    \"\"\"\n",
    "    Traverses the XML directory, extracts DH parameters, and updates\n",
    "    corresponding CSV files in the specified CSV base directory.\n",
    "\n",
    "    Args:\n",
    "        xml_root_directory (str): The path to the directory containing robot XML subfolders (e.g., 'C:\\Desktop\\Robots').\n",
    "        csv_base_directory (str): The path to the directory containing the '_links' folders with the CSVs\n",
    "                                  (e.g., 'C:\\Desktop\\VisualStudioFiles\\RobotSimulator-2025(1)\\RobotSimulator\\RobotSimulator\\Models\\Robot3DModels').\n",
    "    \"\"\"\n",
    "    print(f\"Reading XMLs from: {xml_root_directory}\")\n",
    "    print(f\"Updating CSVs in: {csv_base_directory}\")\n",
    "\n",
    "    # Walk through the XML root directory\n",
    "    for dirpath, dirnames, filenames in os.walk(xml_root_directory):\n",
    "        # We are interested in the immediate subfolders of xml_root_directory\n",
    "        # For example, if xml_root_directory is C:\\Desktop\\Robots, we want C:\\Desktop\\Robots\\ABBIRB120\n",
    "        # Check if the current dirpath is not the root_directory itself\n",
    "        if dirpath == xml_root_directory:\n",
    "            continue # Skip the root directory itself, we want its subfolders\n",
    "\n",
    "        # Get the name of the current subfolder (e.g., 'ABBIRB120')\n",
    "        robot_folder_name = os.path.basename(dirpath)\n",
    "\n",
    "        # Assuming the XML file is named 'Robot.xml' inside each robot folder\n",
    "        robot_xml_path = os.path.join(dirpath, 'Robot.xml')\n",
    "\n",
    "        if os.path.exists(robot_xml_path):\n",
    "            print(f\"Attempting to process XML: {robot_xml_path}\")\n",
    "            xml_robot_name, dh_params = extract_dh_parameters(robot_xml_path)\n",
    "\n",
    "            if xml_robot_name and dh_params:\n",
    "                # Compare the robot name from XML with the folder name\n",
    "                if xml_robot_name == robot_folder_name:\n",
    "                    # Construct the path to the corresponding CSV file\n",
    "                    # CSVs are in 'csv_base_directory/RobotName_links/DH_RobotName.csv'\n",
    "                    csv_folder_name = f\"{robot_folder_name}_links\"\n",
    "                    csv_filename = f\"DH_{robot_folder_name}.csv\"\n",
    "                    \n",
    "                    target_csv_path = os.path.join(csv_base_directory, csv_folder_name, csv_filename)\n",
    "\n",
    "                    if os.path.exists(target_csv_path):\n",
    "                        try:\n",
    "                            with open(target_csv_path, 'w', newline='') as csvfile:\n",
    "                                csv_writer = csv.writer(csvfile)\n",
    "                                \n",
    "                                # First row should be all zeros (6 columns for DH parameters)\n",
    "                                csv_writer.writerow([0] * 6)\n",
    "                                \n",
    "                                # Write the extracted DH parameters\n",
    "                                for row in dh_params:\n",
    "                                    csv_writer.writerow(row)\n",
    "                            print(f\"Successfully updated: {target_csv_path}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error writing to CSV {target_csv_path}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: Corresponding CSV file not found for '{robot_folder_name}' at '{target_csv_path}'. Skipping update.\")\n",
    "                else:\n",
    "                    print(f\"Skipping XML '{robot_xml_path}': Robot name mismatch. XML name: '{xml_robot_name}', Folder name: '{robot_folder_name}'.\")\n",
    "            else:\n",
    "                print(f\"Skipping XML '{robot_xml_path}': Could not extract valid DH parameters or robot name.\")\n",
    "        else:\n",
    "            print(f\"No 'Robot.xml' found in folder: {dirpath}. Skipping.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the new folder containing robot XMLs (e.g., C:\\Desktop\\Robots\\ABBIRB120\\Robot.xml)\n",
    "    xml_source_directory = r'C:\\Desktop\\Robots'\n",
    "    \n",
    "    # Path to the base directory where your _links folders and existing CSVs are located\n",
    "    # (e.g., C:\\Desktop\\VisualStudioFiles\\RobotSimulator-2025(1)\\RobotSimulator\\RobotSimulator\\Models\\Robot3DModels\\ABBIRB120_links\\DH_ABBIRB120.csv)\n",
    "    csv_target_base_directory = r'C:\\Desktop\\VisualStudioFiles\\RobotSimulator-2025(1)\\RobotSimulator\\RobotSimulator\\Models\\Robot3DModels'\n",
    "    \n",
    "    update_dh_csvs_from_xml(xml_source_directory, csv_target_base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9651a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modular system\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "from src.main import (\n",
    "    CarnaticAnnotater, \n",
    "    process_raaga, \n",
    "    analyze_cluster, \n",
    "    label_cluster, \n",
    "    get_cluster_summary\n",
    ")\n",
    "\n",
    "print(\"✅ Modular system imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab93cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Quick one-liner (recommended)\n",
    "print(\"🎵 Running complete analysis pipeline...\")\n",
    "annotater = process_raaga(\"Mayamalavagowlai\", \"Mayamalavagowlai_Vocals\")\n",
    "\n",
    "# Option 2: Step-by-step (if you want more control)\n",
    "# annotater = CarnaticAnnotater(\"Mayamalavagowlai\")\n",
    "# annotater.run_complete_pipeline(\"Mayamalavagowlai_Vocals\")\n",
    "\n",
    "print(\"�� Pipeline completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1294060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overview of all clusters found\n",
    "print(\"📊 Cluster Analysis Summary:\")\n",
    "summary = get_cluster_summary(\"Mayamalavagowlai\")\n",
    "print(f\"Found {len(summary)} clusters:\")\n",
    "print(summary)\n",
    "\n",
    "# Display cluster statistics\n",
    "print(f\"\\n📈 Statistics:\")\n",
    "print(f\"Total segments: {summary['Segment_Count'].sum()}\")\n",
    "print(f\"Average segments per cluster: {summary['Segment_Count'].mean():.1f}\")\n",
    "print(f\"Largest cluster: {summary['Segment_Count'].max()} segments\")\n",
    "print(f\"Smallest cluster: {summary['Segment_Count'].min()} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5794b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the first few clusters\n",
    "print(\"🔍 Analyzing individual clusters...\")\n",
    "\n",
    "# Get list of cluster numbers\n",
    "summary = get_cluster_summary(\"Mayamalavagowlai\")\n",
    "cluster_numbers = summary.index.tolist()\n",
    "\n",
    "# Analyze first 5 clusters (or adjust as needed)\n",
    "for cluster_num in cluster_numbers[:5]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"🎼 Analyzing Cluster {cluster_num}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # This will show plot + play audio\n",
    "    analyze_cluster(\"Mayamalavagowlai\", cluster_num, \"C4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778aa067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exploration - you can modify cluster numbers\n",
    "print(\"🎯 Interactive Cluster Exploration\")\n",
    "print(\"Modify the cluster numbers below to explore different clusters:\")\n",
    "\n",
    "# Example: Analyze specific clusters\n",
    "clusters_to_analyze = [1, 3, 5, 7]  # Change these numbers\n",
    "\n",
    "for cluster_num in clusters_to_analyze:\n",
    "    print(f\"\\n🎵 Analyzing Cluster {cluster_num}\")\n",
    "    analyze_cluster(\"Mayamalavagowlai\", cluster_num, \"C4\")\n",
    "    \n",
    "    # Pause for user input (optional)\n",
    "    input(f\"Press Enter to continue to next cluster...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae650a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After listening to clusters, label them with swara names\n",
    "print(\"��️ Labeling Clusters with Swara Names\")\n",
    "\n",
    "# Example labels (modify based on what you hear)\n",
    "labels_to_apply = {\n",
    "    1: \"Sa Ri Ga\",      # Cluster 1 sounds like Sa Ri Ga\n",
    "    2: \"Pa Da Ni\",      # Cluster 2 sounds like Pa Da Ni\n",
    "    3: \"Sa Pa Sa\",      # Cluster 3 sounds like Sa Pa Sa\n",
    "    4: \"Ga Ma Pa\",      # Cluster 4 sounds like Ga Ma Pa\n",
    "    5: \"Ni Sa Ri\"       # Cluster 5 sounds like Ni Sa Ri\n",
    "}\n",
    "\n",
    "# Apply labels\n",
    "for cluster_num, swara_label in labels_to_apply.items():\n",
    "    print(f\"Labeling cluster {cluster_num} as: {swara_label}\")\n",
    "    label_cluster(\"Mayamalavagowlai\", cluster_num, swara_label)\n",
    "\n",
    "print(\"✅ Labels applied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the updated summary with swara labels\n",
    "print(\"📋 Updated Cluster Summary with Labels:\")\n",
    "updated_summary = get_cluster_summary(\"Mayamalavagowlai\")\n",
    "print(updated_summary)\n",
    "\n",
    "# Show labeled vs unlabeled clusters\n",
    "labeled = updated_summary[updated_summary['Swara'] != 'Not labeled']\n",
    "unlabeled = updated_summary[updated_summary['Swara'] == 'Not labeled']\n",
    "\n",
    "print(f\"\\n��️ Labeled clusters: {len(labeled)}\")\n",
    "print(f\"❓ Unlabeled clusters: {len(unlabeled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into specific clusters\n",
    "print(\"🔬 Detailed Analysis of Specific Clusters\")\n",
    "\n",
    "# Analyze a cluster you're interested in\n",
    "cluster_of_interest = 1  # Change this number\n",
    "\n",
    "print(f\"🎼 Detailed analysis of Cluster {cluster_of_interest}:\")\n",
    "analyze_cluster(\"Mayamalavagowlai\", cluster_of_interest, \"C4\")\n",
    "\n",
    "# Get detailed info about this cluster\n",
    "from src.visualization import ClusterAnalyzer\n",
    "analyzer = ClusterAnalyzer(\"Mayamalavagowlai\")\n",
    "\n",
    "# Load data for detailed analysis\n",
    "import pandas as pd\n",
    "carva_df = pd.read_csv(\"data/output/carva_Mayamalavagowlai.csv\")\n",
    "cluster_data = carva_df[carva_df['Second Labels'] == cluster_of_interest]\n",
    "\n",
    "print(f\"\\n📊 Cluster {cluster_of_interest} Details:\")\n",
    "print(f\"Number of segments: {len(cluster_data)}\")\n",
    "print(f\"From songs: {cluster_data['Index'].nunique()}\")\n",
    "print(f\"Swara label: {cluster_data['Swara'].iloc[0] if len(cluster_data) > 0 else 'Not labeled'}\")\n",
    "\n",
    "# Show segment details\n",
    "print(f\"\\n📄 Segments in this cluster:\")\n",
    "for idx, row in cluster_data.iterrows():\n",
    "    print(f\"  - Song {row['Index']}: frames {row['StartFrame']}-{row['EndFrame']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae428ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clusters that might be similar\n",
    "print(\"🔄 Comparing Similar Clusters\")\n",
    "\n",
    "# Compare clusters that might represent the same swara pattern\n",
    "clusters_to_compare = [1, 3]  # Change these to compare different clusters\n",
    "\n",
    "for cluster_num in clusters_to_compare:\n",
    "    print(f\"\\n🎵 Cluster {cluster_num}:\")\n",
    "    analyze_cluster(\"Mayamalavagowlai\", cluster_num, \"C4\")\n",
    "    \n",
    "    # Add some analysis\n",
    "    summary = get_cluster_summary(\"Mayamalavagowlai\")\n",
    "    if cluster_num in summary.index:\n",
    "        print(f\"  Segments: {summary.loc[cluster_num, 'Segment_Count']}\")\n",
    "        print(f\"  Label: {summary.loc[cluster_num, 'Swara']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export your analysis results\n",
    "print(\"�� Exporting Analysis Results\")\n",
    "\n",
    "# Get final summary\n",
    "final_summary = get_cluster_summary(\"Mayamalavagowlai\")\n",
    "\n",
    "# Export to CSV\n",
    "output_file = \"cluster_analysis_results.csv\"\n",
    "final_summary.to_csv(output_file)\n",
    "print(f\"✅ Results exported to: {output_file}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n📊 Final Analysis Summary:\")\n",
    "print(final_summary)\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\n📈 Final Statistics:\")\n",
    "print(f\"Total clusters analyzed: {len(final_summary)}\")\n",
    "print(f\"Total segments found: {final_summary['Segment_Count'].sum()}\")\n",
    "print(f\"Labeled clusters: {len(final_summary[final_summary['Swara'] != 'Not labeled'])}\")\n",
    "print(f\"Unlabeled clusters: {len(final_summary[final_summary['Swara'] == 'Not labeled'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
